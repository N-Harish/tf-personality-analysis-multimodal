{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f62ab89-88f5-48df-877d-a1db1ed39293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 07:15:44.402370: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "# import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d96ce1-e618-43db-a5da-a29122f377b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4a2f80-fba9-432c-b682-f1938554a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56e90f1-2936-4802-88b6-107b1a8a416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/mount1/miniconda3/envs/tfgpu2.5/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/mount1/miniconda3/envs/tfgpu2.5/lib/python3.8/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "import tensorflow.keras as keras\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import itertools\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "\n",
    "# def unicode_to_ascii(s):\n",
    "#     return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# # def clean_stopwords_shortwords(w):\n",
    "# #     stopwords_list=stopwords.words('english')\n",
    "# #     words = w.split() \n",
    "# #     clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]\n",
    "# #     return \" \".join(clean_words) \n",
    "\n",
    "# def preprocess_sentence(w):\n",
    "#     w = unicode_to_ascii(w.lower().strip())\n",
    "#     w = re.sub(r\"([?.!,¿])\", r\" \", w)\n",
    "#     w = re.sub(r'[\" \"]+', \" \", w)\n",
    "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "#     # w=clean_stopwords_shortwords(w)\n",
    "#     w=re.sub(r'@\\w+', '',w)\n",
    "#     return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f1ddbb-9da0-4b75-a3a7-5d14a3e0efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 07:15:52.199281: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-03 07:15:52.312413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2023-02-03 07:15:52.312494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-02-03 07:15:52.312613: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-02-03 07:15:52.312642: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-02-03 07:15:52.314123: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-03 07:15:52.314601: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-03 07:15:52.315431: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-02-03 07:15:52.316905: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-02-03 07:15:52.316991: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-02-03 07:15:52.322311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880c2d05-e08d-4500-83f4-1c5a33643971",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"first_impression_text_train_annot.csv\")\n",
    "\n",
    "# read test data\n",
    "test_df = pd.read_csv(\"first_impression_text_val_annot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170d593a-61a6-4f55-9257-0b045c6c7b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_path_video</th>\n",
       "      <th>file_path_audio</th>\n",
       "      <th>transcript</th>\n",
       "      <th>filenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_6...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/training80_...</td>\n",
       "      <td>while he's cutting it and then turn around an...</td>\n",
       "      <td>J4GQm9j0JZ0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_7...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/training80_...</td>\n",
       "      <td>responsibility to house the organ I had been ...</td>\n",
       "      <td>zEyRyTnIw5I.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/training80_...</td>\n",
       "      <td>I actually got quite a few sets of black pens...</td>\n",
       "      <td>nskJh7v6v1U.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_3...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/training80_...</td>\n",
       "      <td>I eat a lot and so I'd like a lot of foods. I...</td>\n",
       "      <td>6wHQsN5g2RM.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_7...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/training80_...</td>\n",
       "      <td>ask you guys to leave a question in the comme...</td>\n",
       "      <td>dQOeQYWIgm8.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   extraversion  neuroticism  agreeableness  conscientiousness  interview  \\\n",
       "0      0.523364     0.552083       0.626374           0.601942   0.504673   \n",
       "1      0.345794     0.375000       0.472527           0.582524   0.457944   \n",
       "2      0.252336     0.291667       0.406593           0.485437   0.373832   \n",
       "3      0.457944     0.489583       0.505495           0.398058   0.457944   \n",
       "4      0.607477     0.489583       0.406593           0.621359   0.570093   \n",
       "\n",
       "   openness                                    file_path_video  \\\n",
       "0  0.488889  /mnt/FirstImpression/unzippedData/training80_6...   \n",
       "1  0.366667  /mnt/FirstImpression/unzippedData/training80_7...   \n",
       "2  0.511111  /mnt/FirstImpression/unzippedData/training80_5...   \n",
       "3  0.377778  /mnt/FirstImpression/unzippedData/training80_3...   \n",
       "4  0.622222  /mnt/FirstImpression/unzippedData/training80_7...   \n",
       "\n",
       "                                     file_path_audio  \\\n",
       "0  /mnt/FirstImpression/audio_extract/training80_...   \n",
       "1  /mnt/FirstImpression/audio_extract/training80_...   \n",
       "2  /mnt/FirstImpression/audio_extract/training80_...   \n",
       "3  /mnt/FirstImpression/audio_extract/training80_...   \n",
       "4  /mnt/FirstImpression/audio_extract/training80_...   \n",
       "\n",
       "                                          transcript        filenames  \n",
       "0   while he's cutting it and then turn around an...  J4GQm9j0JZ0.003  \n",
       "1   responsibility to house the organ I had been ...  zEyRyTnIw5I.005  \n",
       "2   I actually got quite a few sets of black pens...  nskJh7v6v1U.004  \n",
       "3   I eat a lot and so I'd like a lot of foods. I...  6wHQsN5g2RM.000  \n",
       "4   ask you guys to leave a question in the comme...  dQOeQYWIgm8.000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128c81d9-b1c6-4686-a7c5-87adb400fc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_path_video</th>\n",
       "      <th>file_path_audio</th>\n",
       "      <th>transcript</th>\n",
       "      <th>filenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/validation8...</td>\n",
       "      <td>It's been a great journey so far. Hopefully w...</td>\n",
       "      <td>modNfUPt3F4.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/validation8...</td>\n",
       "      <td>more pulling movements and then for chest day...</td>\n",
       "      <td>h6LOjpCRXtY.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/validation8...</td>\n",
       "      <td>Eww. But, um, yeah. Some people have been ask...</td>\n",
       "      <td>WER4ww680QQ.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.364486</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.553398</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/validation8...</td>\n",
       "      <td>I am a makeup artist, I am an esthetician, I ...</td>\n",
       "      <td>c4XnKouozXU.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "      <td>/mnt/FirstImpression/audio_extract/validation8...</td>\n",
       "      <td>stops and Minnie agrees yeah it just sucks it...</td>\n",
       "      <td>OEKg-Tvwcbk.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   extraversion  neuroticism  agreeableness  conscientiousness  interview  \\\n",
       "0      0.644860     0.593750       0.615385           0.640777   0.616822   \n",
       "1      0.439252     0.520833       0.417582           0.572816   0.439252   \n",
       "2      0.457944     0.312500       0.428571           0.398058   0.373832   \n",
       "3      0.364486     0.572917       0.527473           0.553398   0.523364   \n",
       "4      0.345794     0.468750       0.516484           0.417476   0.383178   \n",
       "\n",
       "   openness                                    file_path_video  \\\n",
       "0  0.555556  /mnt/FirstImpression/unzippedData/validation80...   \n",
       "1  0.411111  /mnt/FirstImpression/unzippedData/validation80...   \n",
       "2  0.555556  /mnt/FirstImpression/unzippedData/validation80...   \n",
       "3  0.322222  /mnt/FirstImpression/unzippedData/validation80...   \n",
       "4  0.477778  /mnt/FirstImpression/unzippedData/validation80...   \n",
       "\n",
       "                                     file_path_audio  \\\n",
       "0  /mnt/FirstImpression/audio_extract/validation8...   \n",
       "1  /mnt/FirstImpression/audio_extract/validation8...   \n",
       "2  /mnt/FirstImpression/audio_extract/validation8...   \n",
       "3  /mnt/FirstImpression/audio_extract/validation8...   \n",
       "4  /mnt/FirstImpression/audio_extract/validation8...   \n",
       "\n",
       "                                          transcript        filenames  \n",
       "0   It's been a great journey so far. Hopefully w...  modNfUPt3F4.002  \n",
       "1   more pulling movements and then for chest day...  h6LOjpCRXtY.005  \n",
       "2   Eww. But, um, yeah. Some people have been ask...  WER4ww680QQ.004  \n",
       "3   I am a makeup artist, I am an esthetician, I ...  c4XnKouozXU.002  \n",
       "4   stops and Minnie agrees yeah it just sucks it...  OEKg-Tvwcbk.002  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5fffd-e001-4f34-992c-185b2aaff298",
   "metadata": {},
   "source": [
    "# load bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406936fd-3078-459c-bb6f-eb93dbbc4df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722af2a-6939-42e0-87e2-5a261dc95677",
   "metadata": {},
   "source": [
    "# generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b55799-27e4-4088-ae0c-628fb06d7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding_text(df, output_fl = \"bert_feature.pkl\"):\n",
    "    # fill na with empty string\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    \n",
    "    feature = defaultdict(dict)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    for i in tqdm(df.itertuples(), total=len(df)):\n",
    "        # get transcript\n",
    "        sent = i.transcript\n",
    "        \n",
    "        # get input id and mask for bert model\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,add_special_tokens = True,\n",
    "                                              max_length =64,pad_to_max_length = True,\n",
    "                                              return_attention_mask = True)    \n",
    "        # feature dict\n",
    "        feature[i.filenames] = {\"input_ids\": bert_inp['input_ids'], \"attention_mask\": bert_inp['attention_mask']}\n",
    "\n",
    "    with open(output_fl, 'wb') as f:\n",
    "        pkl.dump(dict(feature), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95326931-7c56-4fcd-9d86-b67725e18081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "  0%|                                                                                                             | 0/6000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:py.warnings:/mount1/miniconda3/envs/tfgpu2.5/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:05<00:00, 1014.39it/s]\n"
     ]
    }
   ],
   "source": [
    "extract_embedding_text(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affe200d-65e1-45f0-9c5a-4875f8cae359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d641eb0-6e7a-402a-8f8e-0949b024e92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "07291ee6-eb4f-4cf6-8b47-c9904dca9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_tf_data(filenames, labels):\n",
    "    with open(\"bert_feature.pkl\", 'rb') as f:\n",
    "        train_feature = pkl.load(f)\n",
    "    filenames = filenames.numpy().decode('utf-8')\n",
    "    return np.array(train_feature[filenames]['input_ids']), np.array(train_feature[filenames]['attention_mask']) ,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "68a7385d-d3a5-4aa3-8500-1bf098e410c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_tf_map(filenames, labels):\n",
    "#     return tf.py_function(func=get_feature_tf, inp=[filenames, labels], Tout=[tf.int64, tf.int64, tf.float64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41c077-f893-47f4-9e55-68506b7150ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d8e4fe30-228a-44c6-bb16-393d8de9b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "  0%|                                                                                                             | 0/2000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:py.warnings:/mount1/miniconda3/envs/tfgpu2.5/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1024.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# get test features\n",
    "extract_embedding_text(test_df, output_fl = \"bert_feature_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc111f86-7647-43ef-bfe0-bee852a20efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "99f52021-900d-4691-b59a-1efa2cf5de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test feature\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "with open(\"bert_feature.pkl\", 'rb') as f:\n",
    "    feature = pkl.load(f)\n",
    "\n",
    "    \n",
    "with open(\"bert_feature_test.pkl\", 'rb') as f:\n",
    "    test_feature = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e6c10-736f-4b21-a3d4-47d9d588fbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5395604f-c854-452f-b84f-007ce43434e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf dataset feature for test data\n",
    "def get_feature_tf_data_test(filenames, labels):\n",
    "    with open(\"bert_feature_test.pkl\", 'rb') as f:\n",
    "        train_feature = pkl.load(f)\n",
    "    filenames = filenames.numpy().decode('utf-8')\n",
    "    return np.array(train_feature[filenames]['input_ids']), np.array(train_feature[filenames]['attention_mask']), labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621f436-775f-45aa-8da2-b9e4d4f9e213",
   "metadata": {},
   "source": [
    "# Create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2cfb85e6-5b2f-45d9-8fc4-809732c83357",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = train_df['filenames'].values\n",
    "labels = train_df[['openness',  'conscientiousness', 'extraversion', 'agreeableness','neuroticism']]\n",
    "\n",
    "ft_test = test_df['filenames'].values\n",
    "labels_test = test_df[['openness',  'conscientiousness', 'extraversion', 'agreeableness','neuroticism']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e005cf35-85a9-4499-b325-3ff927ffd767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3626de0e-a58c-4ccf-82a7-b23165f2b6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(feature['J4GQm9j0JZ0.003']['attention_mask']).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d2aaf9a0-17b9-44d6-963c-692064112af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiinp(ft, label):\n",
    "    print(ft.numpy())\n",
    "    return ft, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e12cf108-6a80-437f-8741-c1969d0379f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(ft, labels, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4f1cf0f5-e02a-45fe-a74a-39dc5d2d02c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size and steps per epochs\n",
    "train_batch_size = val_batch_size = 8\n",
    "\n",
    "\n",
    "train_steps = len(train_x)//train_batch_size\n",
    "val_steps = len(test_x)//val_batch_size\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fea65c6e-bdd4-45b1-886c-cb3c42040cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf dataset for training\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "\n",
    "# added tf.py_function to get string from tensor as well to get feature from dict\n",
    "train_ds = train_ds.map(lambda x,y: tf.py_function(get_feature_tf_data, [x, y], [tf.int64, tf.int64, tf.float64]), AUTOTUNE)\n",
    "\n",
    "# for multi input create dict having input name\n",
    "train_ds = train_ds.map(lambda x,y,z : ({\"input_token\": x, \"masked_token\": y}, z))\n",
    "\n",
    "\n",
    "# create tf dataset for validation\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "val_ds = val_ds.map(lambda x,y: tf.py_function(get_feature_tf_data, [x, y], [tf.int64, tf.int64, tf.float64]), AUTOTUNE)\n",
    "# for multi input create dict having input name\n",
    "val_ds = val_ds.map(lambda x,y,z : ({\"input_token\": x, \"masked_token\": y}, z))\n",
    "\n",
    "\n",
    "# create tf dataset for validation\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((ft_test, labels_test))\n",
    "test_ds = test_ds.map(lambda x,y: tf.py_function(get_feature_tf_data_test, [x, y], [tf.int64, tf.int64, tf.float64]), AUTOTUNE)\n",
    "# for multi input create dict having input name\n",
    "test_ds = test_ds.map(lambda x,y,z : ({\"input_token\": x, \"masked_token\": y}, z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce55dc4-726f-4471-9baf-37d346f6e111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f533ed08-60ad-451c-99e3-d29b7d2919f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_token': <tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
      "array([  101,  1045,  2812,  2045,  1005,  1055,  1037,  2843,  1997,\n",
      "        2428,  4658,  4933,  2006,  2026, 10474,  1010,  2061,  2057,\n",
      "        1005,  2128,  6069,  2203,  2009,  2045,  1012,  3246,  2017,\n",
      "        4364,  5632,  1012,  1045,  1005,  2222,  2156,  2017,  2101,\n",
      "        1012,  9765, 22591,  2015,  2203, 10976,   999,   102,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0])>, 'masked_token': <tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>}, <tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.52222222, 0.45631068, 0.48598131, 0.42857143, 0.45833333])>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_ds:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "425442b7-9c42-4a14-88ca-acbf72c940c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in val_ds:\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "202cc0bf-b5eb-4caf-812a-5304bc7523b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(32).repeat().batch(train_batch_size).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.batch(val_batch_size).prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.batch(val_batch_size).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a9cb7-d1fb-4f17-b677-c95255d4cafb",
   "metadata": {},
   "source": [
    "# Generate model architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e4c99e09-6838-4fa0-85bf-a3ba881fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "def mean_acc(y_true, y_pred):\n",
    "    diff = k.abs(y_true - y_pred)\n",
    "    return k.mean(1-diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "61e4276d-8436-4d4f-9b2a-66d6de4040fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_output_dir(outdir, run_desc):\n",
    "    prev_run_dirs = []\n",
    "    if os.path.isdir(outdir):\n",
    "        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(\\\n",
    "            os.path.join(outdir, x))]\n",
    "    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n",
    "    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n",
    "    cur_run_id = max(prev_run_ids, default=-1) + 1\n",
    "    run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{run_desc}')\n",
    "    assert not os.path.exists(run_dir)\n",
    "    os.makedirs(run_dir)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ecea6cef-9390-4d33-87a2-1d0fe46296ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ./checkpoint/00010-text_personality\n"
     ]
    }
   ],
   "source": [
    "outdir = \"./checkpoint/\"\n",
    "run_desc = \"text_personality\"\n",
    "\n",
    "run_dir = generate_output_dir(outdir, run_desc)\n",
    "print(f\"Results saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fcbff84b-14d1-4440-add0-dcd4a433b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "class MyModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch,logs)\n",
    "\n",
    "        # Also save the optimizer state\n",
    "        filepath = self._get_file_path(epoch=epoch, \n",
    "                                       logs=logs, \n",
    "                                       # batch=None\n",
    "                                      )\n",
    "\n",
    "        filepath = filepath.rsplit( \".\", 1 )[ 0 ] \n",
    "        filepath += \".pkl\"\n",
    "\n",
    "        with open(filepath, 'wb') as fp:\n",
    "            pickle.dump(\n",
    "            {\n",
    "                'opt': model.optimizer.get_config(),\n",
    "                'epoch': epoch+1,\n",
    "                'lr': model.optimizer.learning_rate\n",
    "                \n",
    "             # Add additional keys if you need to store more values\n",
    "            }, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('\\nEpoch %05d: saving optimizaer to %s' % (epoch + 1, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "522505d0-5774-4bf7-b915-319e7aca1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "checkpoint = MyModelCheckpoint(os.path.join(run_dir, 'text-personality-model-{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "        monitor='val_loss',verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "# reduce lr on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_acc', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='min', min_lr=0.0000000001)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2e0871a0-66ea-4b1f-b5fc-5553a1ef8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file tf_model.h5 from cache at /home/ayush/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tf_model.h5\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformer_model = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "transformer_model.trainable = True\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(64), name='input_token', dtype=tf.int32)\n",
    "\n",
    "input_masks_ids = tf.keras.layers.Input(shape=(64), name='masked_token', dtype=tf.int32)\n",
    "\n",
    "encodings = transformer_model(input_ids, input_masks_ids)[0]\n",
    "\n",
    "# drop1 = tf.keras.layers.Dropout(0.2)(encodings)\n",
    "\n",
    "avg_pooling = tf.keras.layers.GlobalAvgPool1D()(encodings)\n",
    "\n",
    "\n",
    "output_ly = tf.keras.layers.Dense(5, activation='sigmoid')(avg_pooling)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = output_ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd2221-ab94-4f65-91da-04efc46cf8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "048fd217-9727-44b1-915d-38f8cc0838dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict([val_inp,val_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5d486129-18a1-483c-9f31-d3283215adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_3 (TFBertModel)   TFBaseModelOutputWit 109482240   input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 768)          0           tf_bert_model_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            3845        global_average_pooling1d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3e61aa23-3e1f-4dea-8a66-421083e34356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f78a1969e50>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(\"input_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9c711a2c-54aa-400f-a9c0-9a21855aeebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_3 (TFBertModel)   TFBaseModelOutputWit 109482240   input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 768)          0           tf_bert_model_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            3845        global_average_pooling1d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "log_dir='tensorboard_data/tb_bert'\n",
    "model_save_path='./models/bert_model.h5'\n",
    "\n",
    "print('\\nBert Model',model.summary())\n",
    "\n",
    "# loss = [tf.keras.losses.BinaryCrossentropy(from_logits=False)]\n",
    "loss = ['mae']\n",
    "\n",
    "metric = [mean_acc]\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(loss=loss,optimizer=optimizer,metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b75e1-ce38-4bc8-942d-90f2c344be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/675 [======>.......................] - ETA: 4:40 - loss: 0.1727 - mean_acc: 0.8273"
     ]
    }
   ],
   "source": [
    "model.fit(x= train_ds, steps_per_epoch=train_steps, \n",
    "               validation_data=val_ds,validation_steps=val_steps,\n",
    "               epochs=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e461fa-1195-4c28-bf74-20840661bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history=model.fit([train_inp,train_mask],\n",
    "#                        train_label,batch_size=8,\n",
    "#                        epochs=50,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ac3333b-f3c4-4511-9207-286e4589f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount1/Harish/Personality_trait_Exp/checkpoint/00000-text_personality/text-personality-model-04-0.12.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7547499a-f6d8-4e1d-8a9a-040f2fb860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load trained model state\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import load_model\n",
    "# import pickle\n",
    "\n",
    "# def load_model_data(model_path, opt_path, custom_objects):\n",
    "#     model = load_model(model_path, custom_objects)\n",
    "#     with open(opt_path, 'rb') as fp:\n",
    "#         d = pickle.load(fp)\n",
    "#         epoch = d['epoch']\n",
    "#         opt = d['opt']\n",
    "#     return epoch, model, opt\n",
    "\n",
    "# epoch, model_env, opt = load_model_data('/mount1/mount1/Harish/Personality_trait_Exp/checkpoint/00000-text_personality/text-personality-model-04-0.12.h5',\n",
    "#                                         '/mount1/mount1/Harish/Personality_trait_Exp/checkpoint/00000-text_personality/text-personality-model-04-0.12.pkl', custom_objects={\"mean_acc\": mean_acc, \"TFBertModel\": transformers.TFBertModel})\n",
    "\n",
    "# model_env.compile(\n",
    "#     loss = ['mae'],\n",
    "#     # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "#     optimizer = tf.keras.optimizers.Adam.from_config(opt), \n",
    "#     metrics = [mean_acc]\n",
    "# )\n",
    "\n",
    "# model_env.fit([train_inp,train_mask],\n",
    "#               train_label,batch_size=8,\n",
    "#               epochs=50, validation_data=([val_inp,val_mask],val_label),initial_epoch=epoch, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367865d-6732-451c-8ef4-3a537c485292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0943b798-07fb-4726-9a9d-742bf9774727",
   "metadata": {},
   "source": [
    "# Saving best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f9b1c74-bb0d-491b-9914-a7ae27594cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount1/Harish/Personality_trait_Exp/checkpoint/00001-text_personality/text-personality-model-10-0.12.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec19d3b9-7e22-4389-97dd-1ef17c0b01ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "# import tensorflow.keras.backend as k\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# def mean_acc(y_true, y_pred):\n",
    "#     diff = k.abs(y_true - y_pred)\n",
    "#     return k.mean(1-diff)\n",
    "\n",
    "\n",
    "# model_path = \"/mount1/mount1/Harish/Personality_trait_Exp/checkpoint/00003-text_personality/text-personality-model-08-0.12.h5\"\n",
    "\n",
    "# custom_objects={\"mean_acc\": mean_acc, \"TFBertModel\": transformers.TFBertModel}\n",
    "# model = load_model(model_path, custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "950f59f4-62a8-46de-a6e2-9b20c10368fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_4 (TFBertModel)   TFBaseModelOutputWit 109482240   input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf_bert_model_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            3845        global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6afc0c43-6566-48de-9e9f-e13896ced933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"text_personality_ocean.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "777aacba-0484-4e81-8fe5-733523f8ae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-01 15:38:20.258704: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1050). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text_personality_ocean/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text_personality_ocean/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"text_personality_ocean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c01f8-4f3c-4bc4-baa6-895954bc4d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu2.5",
   "language": "python",
   "name": "tfgpu2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
