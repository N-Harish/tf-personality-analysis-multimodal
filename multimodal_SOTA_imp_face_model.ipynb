{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f62ab89-88f5-48df-877d-a1db1ed39293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:00:54.325013: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet101V2, ResNet50V2\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "# from aligned_face import align_face_mediapipe\n",
    "import facealignment\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# cuda = torch.device('cuda')\n",
    "# mtcnn = MTCNN(post_process=False, device=cuda)\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da961fe6-ecdd-4890-8468-26273b0565ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785821ae-aa4a-4420-b9ad-a6d2f0a800c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6c6fd-d197-40d2-bbf5-86f9052a12f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adffd3c-736e-41f9-87ad-f057dbdf9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7554257f-e3e5-44e8-a878-8cec7ddfdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Choose the `slow_r50` model \n",
    "\n",
    "# model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd08d657-0f67-4a42-8421-982153be5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import urllib\n",
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# from torchvision.transforms import Compose, Lambda\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9c9da7-8f50-4151-b8ef-7bc26a19572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set to GPU or CPU\n",
    "# device = \"cpu\"\n",
    "# model = model.eval()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1541e1-6386-44e9-9f16-63d60317eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# json_filename = \"kinetics_classnames.json\"\n",
    "# try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "# except: urllib.request.urlretrieve(json_url, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68b0911-3c11-4667-9df0-8c58307b162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(json_filename, \"r\") as f:\n",
    "#     kinetics_classnames = json.load(f)\n",
    "\n",
    "# # Create an id to label name mapping\n",
    "# kinetics_id_to_classname = {}\n",
    "# for k, v in kinetics_classnames.items():\n",
    "#     kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeac0ef0-7d76-496c-9f5d-2e029eabe214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side_size = 256\n",
    "# mean = [0.45, 0.45, 0.45]\n",
    "# std = [0.225, 0.225, 0.225]\n",
    "# crop_size = 256\n",
    "# num_frames = 8\n",
    "# sampling_rate = 8\n",
    "# frames_per_second = 30\n",
    "\n",
    "# # Note that this transform is specific to the slow_R50 model.\n",
    "# transform =  ApplyTransformToKey(\n",
    "#     key=\"video\",\n",
    "#     transform=Compose(\n",
    "#         [\n",
    "#             UniformTemporalSubsample(num_frames),\n",
    "#             Lambda(lambda x: x/255.0),\n",
    "#             NormalizeVideo(mean, std),\n",
    "#             ShortSideScale(\n",
    "#                 size=side_size\n",
    "#             ),\n",
    "#             CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "#         ]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # The duration of the input clip is also specific to the model.\n",
    "# clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f735d634-e170-402a-8f91-3900a6571596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
    "# video_path = 'archery.mp4'\n",
    "# try: urllib.URLopener().retrieve(url_link, video_path)\n",
    "# except: urllib.request.urlretrieve(url_link, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07672184-d9d3-4d93-8eb4-604c1ad6fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = \"/mount1/mount1/First-Impression/Apparent_traits/DeepImpression/deep_impression/_fJcW5234bY.001.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d8890c-3727-417a-b125-709748110065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the duration of the clip to load by specifying the start and end duration\n",
    "# # The start_sec should correspond to where the action occurs in the video\n",
    "# start_sec = 0\n",
    "# end_sec = start_sec + clip_duration\n",
    "\n",
    "# # Initialize an EncodedVideo helper class and load the video\n",
    "# video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# # Load the desired clip\n",
    "# video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e20a17c-828e-409b-91e6-81e297b7d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0b334a-cee1-4ca4-99a6-78842cbbe31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Apply a transform to normalize the video input\n",
    "# video_data = transform(video_data)\n",
    "\n",
    "# # Move the inputs to the desired device\n",
    "# inputs = video_data[\"video\"]\n",
    "# inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c147d433-4ed2-4e0a-8c09-8062d296f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pass the input clip through the model\n",
    "# preds = model(inputs[None, ...])\n",
    "\n",
    "# # Get the predicted classes\n",
    "# post_act = torch.nn.Softmax(dim=1)\n",
    "# preds = post_act(preds)\n",
    "# pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# # Map the predicted classes to the label names\n",
    "# pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "# print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db03bf12-9765-4923-9b58-9e00fa5728df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# cnt  =0\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "# while True:\n",
    "#     ret, fr = cap.read()\n",
    "#     if ret:\n",
    "#         cnt += 1\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d4ad8d0-a83b-4c0d-83a7-fb5fc6e4fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e45bad6-b6d4-4fb7-a0c7-bb1f37d5b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76be6d46-dc44-426d-a509-8d8d88165345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:00:57.985830: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-31 15:00:58.043169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2023-01-31 15:00:58.043221: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-31 15:00:58.043304: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-31 15:00:58.043330: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-01-31 15:00:58.044581: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-31 15:00:58.044992: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-31 15:00:58.045754: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-01-31 15:00:58.047215: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-31 15:00:58.047303: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-31 15:00:58.052275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6000c50-a5e0-4fe0-8bd2-4d46f22b3714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:00:58.078757: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 15:00:58.234338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2023-01-31 15:00:58.238467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-01-31 15:00:58.238575: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-31 15:01:00.645878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-31 15:01:00.645918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-01-31 15:01:00.645927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-01-31 15:01:00.653942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 38317 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0)\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50V2(include_top=False, weights=\"imagenet\",input_shape=(224,224, 3), pooling='max')\n",
    "# base_model.trainble = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "382f8d53-498d-4c8d-95e3-4a87b8a3edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04c64bc6-1f2b-48a9-b143-bf3f2ea1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# emb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77c85ffa-361e-4077-96b1-46f1a0515a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(video_pth, seq_len=30, target_resolution=(224, 224)):\n",
    "#     \"\"\"\n",
    "#     extract frames and audio from the video,\n",
    "#     store the cropped frames and audio file in the output folders\n",
    "#     seq_len: how many frames will be extracted from the video.\n",
    "#               Considering all videos from this dataset have similar duration\n",
    "#               video_duration = seq_len / fps\n",
    "#     target_resolution: (desired_height, desired_width) of the facial frame extracted\n",
    "#     \"\"\"\n",
    "#     frames = []\n",
    "#     video = VideoFileClip(video_pth, target_resolution=target_resolution)\n",
    "    \n",
    "#     times = list(np.arange(0, video.duration, video.duration/seq_len))\n",
    "#     if len(times) < seq_len:\n",
    "#         times.append(video.duration)\n",
    "#     times = times[:seq_len]\n",
    "\n",
    "#     for i, t in enumerate(times):\n",
    "#         img = cv2.cvtColor(video.get_frame(t), cv2.COLOR_BGR2RGB)\n",
    "#         frames.append(img)\n",
    "#         img1 = np.expand_dims(img, 0)\n",
    "#         # print(img1.shape)\n",
    "#         em = base_model.predict(img1)\n",
    "#         emb.append(em)\n",
    "#     print(\"Video duration {} seconds. Extracted {} frames\".format(video.duration, len(times)))\n",
    "    \n",
    "# # preprocess(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61fef3d-06fe-4afa-a69f-39e8116b62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43ba64b4-5af6-4b01-9623-50cfd5075d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 30\n",
    "h = w =224\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55da9c-b54d-4db5-a70b-7f6fbc741013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "affc6e41-2aaf-46cc-8c18-bd2929e668ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeDistributed Layer is used to run resnet model first on all the frames and then run LSTM on top of it\n",
    "input_layer_face = Input(shape=(time_step, h, w, c))\n",
    "\n",
    "x = TimeDistributed(base_model)(input_layer_face)\n",
    "\n",
    "x = LSTM(256)(x)\n",
    "\n",
    "x =  Dense(5, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(input_layer_face, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6556c480-42d2-4e65-aaba-ac2dd2ffd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "def mean_acc(y_true, y_pred):\n",
    "    diff = k.abs(y_true - y_pred)\n",
    "    return k.mean(1-diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76012503-8c34-40a5-8db5-2d295c30a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = ['mae'],\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "    metrics = [mean_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "807373bd-24fb-474e-930f-8644d2753acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 30, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 30, 2048)          23564800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               2360320   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 25,926,405\n",
      "Trainable params: 25,880,965\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0075b-7e10-4bb4-9ed1-d18b11dace30",
   "metadata": {},
   "source": [
    "# Dataset preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa9cf18-e215-4cee-9c02-8099a519b31c",
   "metadata": {},
   "source": [
    "## Training data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9838a47-d78f-45a6-bf1b-3fed60f0a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_file_path(fl):\n",
    "    train_fol = \"/mnt/FirstImpression/unzippedData/**/*.mp4\"\n",
    "    train_fol = glob(train_fol)\n",
    "    train_fol = [i for i in train_fol if \"training\" in Path(i).parts[-2]]\n",
    "    fil = list(filter(lambda x: fl in x, train_fol))\n",
    "    return fil[0]\n",
    "#     for i in train_fol:\n",
    "#         if fl in i:\n",
    "#             fil = i\n",
    "            \n",
    "#     return fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58bbe46f-58f8-4cc3-a342-be4f5616e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_annot = pd.read_pickle(\"/mnt/FirstImpression/annotation_training.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d55fb6d8-c96c-40e9-8dc0-c95453a66bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation = pd.DataFrame().from_dict(train_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1132d89-68e9-4484-9fa9-b80eb0efb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation['file_name'] = train_annotation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84a230fc-26ea-43c0-b385-228f7168c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_annotation['file_path'] = train_annotation['file_name'].progress_apply(lambda x: get_train_file_path(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "336f6965-96b6-4ab0-b35b-e91678fb43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_annotation.to_csv(\"first_impression_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a69c467a-d2aa-4537-b288-5986a8d8fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation = pd.read_csv(\"first_impression_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef428d1a-0912-484f-825b-13686ffc2a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>J4GQm9j0JZ0.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>zEyRyTnIw5I.005.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>nskJh7v6v1U.004.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>6wHQsN5g2RM.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>dQOeQYWIgm8.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>Eh7WRYXVh9M.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>0.728972</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>2q8orkMs2Jg.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>0.700935</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>F1lAPYh4t3U.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>0.317757</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>cxJ0u6r0-pU.001.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>0.401869</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>hfUH9Am-Izs.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      extraversion  neuroticism  agreeableness  conscientiousness  interview  \\\n",
       "0         0.523364     0.552083       0.626374           0.601942   0.504673   \n",
       "1         0.345794     0.375000       0.472527           0.582524   0.457944   \n",
       "2         0.252336     0.291667       0.406593           0.485437   0.373832   \n",
       "3         0.457944     0.489583       0.505495           0.398058   0.457944   \n",
       "4         0.607477     0.489583       0.406593           0.621359   0.570093   \n",
       "...            ...          ...            ...                ...        ...   \n",
       "5995      0.523364     0.479167       0.626374           0.621359   0.588785   \n",
       "5996      0.728972     0.760417       0.582418           0.524272   0.616822   \n",
       "5997      0.700935     0.770833       0.747253           0.699029   0.691589   \n",
       "5998      0.317757     0.531250       0.582418           0.679612   0.616822   \n",
       "5999      0.401869     0.500000       0.461538           0.543689   0.429907   \n",
       "\n",
       "      openness            file_name  \\\n",
       "0     0.488889  J4GQm9j0JZ0.003.mp4   \n",
       "1     0.366667  zEyRyTnIw5I.005.mp4   \n",
       "2     0.511111  nskJh7v6v1U.004.mp4   \n",
       "3     0.377778  6wHQsN5g2RM.000.mp4   \n",
       "4     0.622222  dQOeQYWIgm8.000.mp4   \n",
       "...        ...                  ...   \n",
       "5995  0.544444  Eh7WRYXVh9M.000.mp4   \n",
       "5996  0.822222  2q8orkMs2Jg.003.mp4   \n",
       "5997  0.788889  F1lAPYh4t3U.000.mp4   \n",
       "5998  0.588889  cxJ0u6r0-pU.001.mp4   \n",
       "5999  0.588889  hfUH9Am-Izs.000.mp4   \n",
       "\n",
       "                                              file_path  \n",
       "0     /mnt/FirstImpression/unzippedData/training80_6...  \n",
       "1     /mnt/FirstImpression/unzippedData/training80_7...  \n",
       "2     /mnt/FirstImpression/unzippedData/training80_5...  \n",
       "3     /mnt/FirstImpression/unzippedData/training80_3...  \n",
       "4     /mnt/FirstImpression/unzippedData/training80_7...  \n",
       "...                                                 ...  \n",
       "5995  /mnt/FirstImpression/unzippedData/training80_5...  \n",
       "5996  /mnt/FirstImpression/unzippedData/training80_5...  \n",
       "5997  /mnt/FirstImpression/unzippedData/training80_6...  \n",
       "5998  /mnt/FirstImpression/unzippedData/training80_1...  \n",
       "5999  /mnt/FirstImpression/unzippedData/training80_4...  \n",
       "\n",
       "[6000 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fb9daec-5938-4cfa-b53a-af28d8c09c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "\n",
    "# with open(\"./first_impression_train.pkl\", 'wb') as f:\n",
    "#     pkl.dump(train_annot, f)\n",
    "\n",
    "# # pd.to_pickle(\"./first_impression_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddfa3383-cf9a-4f8f-a137-012a505c3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_pth, output_pth, seq_len=30, target_resolution=(224, 224)):\n",
    "    \"\"\"\n",
    "    extract frames and audio from the video,\n",
    "    store the cropped frames and audio file in the output folders\n",
    "    seq_len: how many frames will be extracted from the video.\n",
    "              Considering all videos from this dataset have similar duration\n",
    "              video_duration = seq_len / fps\n",
    "    target_resolution: (desired_height, desired_width) of the facial frame extracted\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    video = VideoFileClip(video_pth, target_resolution=target_resolution)\n",
    "    \n",
    "    times = list(np.arange(0, video.duration, video.duration/seq_len))\n",
    "    if len(times) < seq_len:\n",
    "        times.append(video.duration)\n",
    "    times = times[:seq_len]\n",
    "    \n",
    "    for i, t in enumerate(times):\n",
    "        img = cv2.cvtColor(video.get_frame(t), cv2.COLOR_BGR2RGB)\n",
    "        # frames.append(img)\n",
    "        # img1 = np.expand_dims(img, 0)\n",
    "        # print(img1.shape)\n",
    "        # em = base_model.predict(img1)\n",
    "        # emb.append(em)\n",
    "        cv2.imwrite(f\"{output_pth}/frame_{i}.jpg\", img)\n",
    "    # print(\"Video duration {} seconds. Extracted {} frames\".format(video.duration, len(times)))\n",
    "\n",
    "# fet_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d45200a-4e65-4c7f-9b85-92f1863198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "\n",
    "# # make training folder\n",
    "# train_folder = \"training_frame_folder\"\n",
    "\n",
    "# if not os.path.isdir(train_folder):\n",
    "#     os.mkdir(train_folder)\n",
    "\n",
    "# else:\n",
    "#     train_fol = \"/mount1/mount1/First-Impression/unzippedData/**/*.mp4\"\n",
    "#     train_fol = glob(train_fol)\n",
    "#     train_fol = [i for i in train_fol if \"training\" in Path(i).parts[-2]]\n",
    "    \n",
    "#     for fl in tqdm(train_fol):\n",
    "#         img_folder = train_folder+\"/\" + Path(fl).parts[-1].split(\".\")[0]\n",
    "        \n",
    "#         # print(img_folder)\n",
    "        \n",
    "#         if not os.path.isdir(img_folder):\n",
    "#             os.mkdir(img_folder)\n",
    "#         get_frames(video_pth=fl, output_pth=img_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd95e5d-5aba-4d2e-acd6-402fefc60ab5",
   "metadata": {},
   "source": [
    "## val data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d892c6-b355-4cc8-8bb7-974d801f5238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1caf951-95d0-410e-9be0-ba6036402221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_file_path(fl):\n",
    "    train_fol = \"/mnt/FirstImpression/unzippedData/**/*.mp4\"\n",
    "    train_fol = glob(train_fol)\n",
    "    train_fol = [i for i in train_fol if \"validation\" in Path(i).parts[-2]]\n",
    "    fil = list(filter(lambda x: fl in x, train_fol))\n",
    "    return fil[0]\n",
    "#     for i in train_fol:\n",
    "#         if fl in i:\n",
    "#             fil = i\n",
    "            \n",
    "#     return fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6205c95a-7f35-4a1b-a2ad-f2fd64b33b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_annot = pd.read_pickle(\"/mnt/FirstImpression/annotation_validation.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49752d07-5c06-4286-8c7d-b436730dbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotation = pd.DataFrame().from_dict(val_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ab5cca8-98a3-4de1-a8cd-4f9979ac471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotation['file_name'] = val_annotation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23f2be18-f466-4fe6-b90e-22f9262b0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_annotation['file_path'] = val_annotation['file_name'].progress_apply(lambda x: get_val_file_path(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4fdfc06-2906-4b40-ad59-51b18882e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_annotation.to_csv(\"first_impression_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed40af2f-86c0-406c-8384-a3bc63e40f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotation = pd.read_csv(\"first_impression_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cebf0fd-6eef-4160-bda4-85af49f21ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>modNfUPt3F4.002.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>h6LOjpCRXtY.005.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>WER4ww680QQ.004.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.364486</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.553398</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>c4XnKouozXU.002.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>OEKg-Tvwcbk.002.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.614583</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.626168</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>3LAaFUSGvsU.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>n2BuwHbdilY.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.551402</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>GcuoyJPO-KU.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.551402</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>uf_sIIw4zxY.004.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.560748</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.635514</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>jd9_8OPxM3A.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      extraversion  neuroticism  agreeableness  conscientiousness  interview  \\\n",
       "0         0.644860     0.593750       0.615385           0.640777   0.616822   \n",
       "1         0.439252     0.520833       0.417582           0.572816   0.439252   \n",
       "2         0.457944     0.312500       0.428571           0.398058   0.373832   \n",
       "3         0.364486     0.572917       0.527473           0.553398   0.523364   \n",
       "4         0.345794     0.468750       0.516484           0.417476   0.383178   \n",
       "...            ...          ...            ...                ...        ...   \n",
       "1995      0.570093     0.614583       0.494505           0.689320   0.626168   \n",
       "1996      0.542056     0.541667       0.549451           0.669903   0.579439   \n",
       "1997      0.551402     0.593750       0.560440           0.572816   0.504673   \n",
       "1998      0.514019     0.552083       0.461538           0.572816   0.551402   \n",
       "1999      0.560748     0.635417       0.725275           0.621359   0.635514   \n",
       "\n",
       "      openness            file_name  \\\n",
       "0     0.555556  modNfUPt3F4.002.mp4   \n",
       "1     0.411111  h6LOjpCRXtY.005.mp4   \n",
       "2     0.555556  WER4ww680QQ.004.mp4   \n",
       "3     0.322222  c4XnKouozXU.002.mp4   \n",
       "4     0.477778  OEKg-Tvwcbk.002.mp4   \n",
       "...        ...                  ...   \n",
       "1995  0.577778  3LAaFUSGvsU.000.mp4   \n",
       "1996  0.666667  n2BuwHbdilY.000.mp4   \n",
       "1997  0.644444  GcuoyJPO-KU.003.mp4   \n",
       "1998  0.733333  uf_sIIw4zxY.004.mp4   \n",
       "1999  0.666667  jd9_8OPxM3A.003.mp4   \n",
       "\n",
       "                                              file_path  \n",
       "0     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "2     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "3     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "4     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "...                                                 ...  \n",
       "1995  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1996  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1997  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1998  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1999  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "\n",
       "[2000 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91469b2c-b855-49ce-96c2-214433f7c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "\n",
    "# with open(\"./first_impression_val.pkl\", 'wb') as f:\n",
    "#     pkl.dump(val_annot, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ddccf-2499-4cb8-8dff-66dc00748638",
   "metadata": {},
   "source": [
    "# Custom callback to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98d2eac6-5a9b-425b-88eb-2e60f1a7fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_dir(outdir, run_desc):\n",
    "    prev_run_dirs = []\n",
    "    if os.path.isdir(outdir):\n",
    "        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(\\\n",
    "            os.path.join(outdir, x))]\n",
    "    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n",
    "    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n",
    "    cur_run_id = max(prev_run_ids, default=-1) + 1\n",
    "    run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{run_desc}')\n",
    "    assert not os.path.exists(run_dir)\n",
    "    os.makedirs(run_dir)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14886ae0-3164-4839-a20c-8afafc96d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ./checkpoint/face_personality/00008-face_personality\n"
     ]
    }
   ],
   "source": [
    "outdir = \"./checkpoint/face_personality/\"\n",
    "run_desc = \"face_personality\"\n",
    "\n",
    "run_dir = generate_output_dir(outdir, run_desc)\n",
    "print(f\"Results saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7885df8a-530e-40f5-9331-e9ebd55c3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "class MyModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch,logs)\n",
    "\n",
    "        # Also save the optimizer state\n",
    "        filepath = self._get_file_path(epoch=epoch, \n",
    "                                       logs=logs\n",
    "                                       # ,batch=None\n",
    "                                      )\n",
    "\n",
    "        filepath = filepath.rsplit( \".\", 1 )[ 0 ] \n",
    "        filepath += \".pkl\"\n",
    "\n",
    "        with open(filepath, 'wb') as fp:\n",
    "            pickle.dump(\n",
    "            {\n",
    "                'opt': model.optimizer.get_config(),\n",
    "                'epoch': epoch+1,\n",
    "                'lr': model.optimizer.learning_rate\n",
    "                \n",
    "             # Add additional keys if you need to store more values\n",
    "            }, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('\\nEpoch %05d: saving optimizaer to %s' % (epoch + 1, filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea19598-be73-4438-9b03-37e1de3783b1",
   "metadata": {},
   "source": [
    "# Tf dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4dbe20c-cc38-4866-8455-9d3d905ddca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "        Pad and resize an image from a video.\n",
    "\n",
    "        Args:\n",
    "          frame: Image that needs to resized and padded. \n",
    "          output_size: Pixel size of the output frame image.\n",
    "\n",
    "        Return:\n",
    "          Formatted frame with padding of specified output size.\n",
    "    \"\"\"\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return tf.keras.applications.resnet_v2.preprocess_input(frame)\n",
    "    # return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b26b406-00d8-4deb-8ad9-cc740ad55d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "tools = facealignment.FaceAlignmentTools()\n",
    "\n",
    "\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "    \"\"\"\n",
    "        Creates frames from each video file present for each category.\n",
    "\n",
    "        Args:\n",
    "          video_path: File path to the video.\n",
    "          n_frames: Number of frames to be created per video file.\n",
    "          output_size: Pixel size of the output frame image.\n",
    "\n",
    "        Return:\n",
    "          An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    if ret:\n",
    "        st = time()\n",
    "        face = tools.align(frame)\n",
    "        # face = mtcnn(frame)  \n",
    "        # print(f\"Time to extract aligned face from first frame:- {time() - st}\")\n",
    "        \n",
    "        if face is not None:\n",
    "            ft_time = time()\n",
    "            result.append(format_frames(face, output_size))\n",
    "            # print(f\"Time for formating frame:- {time() - ft_time}\")\n",
    "        else:\n",
    "            er_t = time()\n",
    "            result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "            # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "    else:\n",
    "        er_t = time()\n",
    "        result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "        # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "\n",
    "    s = time()\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            face = tools.align(frame)\n",
    "            # face = mtcnn(frame)\n",
    "            \n",
    "            if face is not None:\n",
    "                ft_time = time()\n",
    "                face = format_frames(face, output_size)\n",
    "                result.append(face)\n",
    "                # print(f\"Time for formating frame:- {time() - ft_time}\")\n",
    "            else:\n",
    "                er_t = time()\n",
    "                result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "                # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "        else:\n",
    "            er_t = time()\n",
    "            result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "            # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "                \n",
    "    # print(f\"Time for getting aligned face from n-1 frame:- {time() - s}\")\n",
    "    src.release()\n",
    "    if len(result) < n_frames:\n",
    "        print(len(result))\n",
    "    \n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c49b0-ffa0-4764-8e7b-b2de206c7bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1529e377-6806-44f6-bb72-aa730ae1508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligned_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8dafde91-a857-41da-9793-b202c642d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = train_annotation.file_path[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da5101a6-6850-48ec-ba99-e4b5b5208116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 224, 224, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_from_video_file(video_path, 30).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4047c5ed-558a-408e-a4fa-1fbb61feed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample df to test model working\n",
    "df = train_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "163d887c-0768-4fdd-8de5-bd70de6ab019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50a81571-82ef-4768-a319-c1d55bd9abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ee603ec-c586-4ea1-be54-9a5005ea796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(n=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3669d710-872e-4899-a888-cf6ed55b9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0471e465-068f-40f8-ac75-4e53e5fc4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcce6e9d-5146-4f02-8a56-49964d887932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61e48bf6-4856-4003-bd5c-055d65729680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d34c8a8-e2c2-4cc6-91b5-b37b93bfbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f7a7a36-e4ca-4e03-a580-9f964a56afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fr_and_label_from_video(video_path,label):\n",
    "#     \"\"\"\n",
    "#         Creates frames from each video file present for each category.\n",
    "\n",
    "#         Args:\n",
    "#           video_path: File path to the video.\n",
    "#           n_frames: Number of frames to be created per video file.\n",
    "#           output_size: Pixel size of the output frame image.\n",
    "\n",
    "#         Return:\n",
    "#           An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     n_frames=30\n",
    "#     output_size = (224,224)\n",
    "#     frame_step = 15\n",
    "    \n",
    "#     # print(tf.strings.as_string(video_path))\n",
    "#     print(video_path)\n",
    "#     print(label)\n",
    "    \n",
    "#     # Read each video frame by frame\n",
    "#     result = []\n",
    "#     src = cv2.VideoCapture(str(video_path))  \n",
    "#     print(src.isOpened())\n",
    "#     video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "#     need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "#     if need_length > video_length:\n",
    "#         start = 0\n",
    "#     else:\n",
    "#         max_start = video_length - need_length\n",
    "#         start = random.randint(0, max_start + 1)\n",
    "\n",
    "#     src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "#     # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "#     ret, frame = src.read()\n",
    "#     result.append(format_frames(frame, output_size))\n",
    "#     for _ in range(n_frames - 1):\n",
    "#         for _ in range(frame_step):\n",
    "#             ret, frame = src.read()\n",
    "#         if ret:\n",
    "#             frame = format_frames(frame, output_size)\n",
    "#             result.append(frame)\n",
    "#         else:\n",
    "#             result.append(np.zeros_like(result[0]))\n",
    "#     src.release()\n",
    "#     result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "#     return result, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c12fab-ca85-4aa2-9846-df8eb0dfcf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6173ed6-8d57-41c5-a3d7-89151c5c7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seperate x_train  and y_train\n",
    "\n",
    "# y_train = train_df[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']].values\n",
    "# x_train = train_df.file_path.values\n",
    "\n",
    "# tr_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d313c7d-f9b1-4da9-8ce1-93016952fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = val_batch_size = 8\n",
    "\n",
    "\n",
    "train_steps = len(train_df)//train_batch_size\n",
    "val_steps = len(test_df)//val_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7ba4516-847f-48b1-8b89-21f62f9ce5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf dataset generator\n",
    "class FrameGenerator:\n",
    "    def __init__(self, df, n_frames, training = False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "            Args:\n",
    "                path: Video file paths.\n",
    "                n_frames: Number of frames. \n",
    "                training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "  \n",
    "\n",
    "    def __call__(self):\n",
    "        if self.training:\n",
    "            import sklearn\n",
    "            self.df = sklearn.utils.shuffle(self.df)\n",
    "            \n",
    "        for files in self.df.itertuples():\n",
    "            video_frames = frames_from_video_file(files.file_path, self.n_frames)\n",
    "            openness = files.openness\n",
    "            conscientiousness = files.conscientiousness\n",
    "            extraversion = files.extraversion\n",
    "            agreeableness = files.agreeableness\n",
    "            neuroticism = files.neuroticism\n",
    "            \n",
    "            label = np.array([files.openness, files.conscientiousness ,files.extraversion, files.agreeableness, files.neuroticism])\n",
    "            \n",
    "            yield video_frames, label\n",
    "            # return video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ec83f31-c8ab-43f5-999e-3e7246057fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf dataset generator\n",
    "class MetaGenerator:\n",
    "    def __init__(self, df, n_frames, training = False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "            Args:\n",
    "                path: Video file paths.\n",
    "                n_frames: Number of frames. \n",
    "                training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "  \n",
    "\n",
    "    def __call__(self):\n",
    "        if self.training:\n",
    "            import sklearn\n",
    "            self.df = sklearn.utils.shuffle(self.df)\n",
    "            \n",
    "        for files in self.df.itertuples():\n",
    "            # video_frames = frames_from_video_file(files.file_path, self.n_frames)\n",
    "            video_frames = files.file_path\n",
    "            openness = files.openness\n",
    "            conscientiousness = files.conscientiousness\n",
    "            extraversion = files.extraversion\n",
    "            agreeableness = files.agreeableness\n",
    "            neuroticism = files.neuroticism\n",
    "            \n",
    "            label = np.array([files.openness, files.conscientiousness ,files.extraversion, files.agreeableness, files.neuroticism])\n",
    "            \n",
    "            yield video_frames, label\n",
    "            # return video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2895c600-aa7b-4542-9e04-4ea35868d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "tools = facealignment.FaceAlignmentTools()\n",
    "\n",
    "\n",
    "\n",
    "def frames_from_video_file_map(video_path, label, n_frames, output_size = (224,224), frame_step = 15):\n",
    "    \"\"\"\n",
    "        Creates frames from each video file present for each category.\n",
    "\n",
    "        Args:\n",
    "          video_path: File path to the video.\n",
    "          n_frames: Number of frames to be created per video file.\n",
    "          output_size: Pixel size of the output frame image.\n",
    "\n",
    "        Return:\n",
    "          An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    if ret:\n",
    "        st = time()\n",
    "        face = tools.align(frame)\n",
    "        # face = mtcnn(frame)  \n",
    "        # print(f\"Time to extract aligned face from first frame:- {time() - st}\")\n",
    "        \n",
    "        if face is not None:\n",
    "            ft_time = time()\n",
    "            result.append(format_frames(face, output_size))\n",
    "            # print(f\"Time for formating frame:- {time() - ft_time}\")\n",
    "        else:\n",
    "            er_t = time()\n",
    "            result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "            # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "    else:\n",
    "        er_t = time()\n",
    "        result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "        # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "\n",
    "    s = time()\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            face = tools.align(frame)\n",
    "            # face = mtcnn(frame)\n",
    "            \n",
    "            if face is not None:\n",
    "                ft_time = time()\n",
    "                face = format_frames(face, output_size)\n",
    "                result.append(face)\n",
    "                # print(f\"Time for formating frame:- {time() - ft_time}\")\n",
    "            else:\n",
    "                er_t = time()\n",
    "                result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "                # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "        else:\n",
    "            er_t = time()\n",
    "            result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "            # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "                \n",
    "    # print(f\"Time for getting aligned face from n-1 frame:- {time() - s}\")\n",
    "    src.release()\n",
    "    if len(result) < n_frames:\n",
    "        print(len(result))\n",
    "    \n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "    return result, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a7a0d-8305-4b1a-b4ee-e25e255908fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bf6f9-3f39-4b62-8e55-06823d3a2324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5dad692-70dc-4e81-8726-b2d10a846100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_from_video_file(train_df.file_path[1], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "facc7d33-4a11-4f1a-9c4f-216d842ba73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_ds.take(10):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ad7ddcd-7b3d-4b4f-8650-e4906ab30ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_steps = 30\n",
    "\n",
    "\n",
    "# def get_batches(df, b_size = 16, train=False):\n",
    "#     count = 0 # counter variable \n",
    "    \n",
    "#     if train:\n",
    "#         import sklearn\n",
    "#         df = sklearn.utils.shuffle(df)\n",
    "            \n",
    "#     # looping over all videos\n",
    "#     while count < len(df): \n",
    "#         all_frames = np.empty((0,time_steps,h,w,c)) # numpy array of batch of videos \n",
    "#         labels = np.empty((0,5)) # numpy array to contain the 5 personality scores \n",
    "\n",
    "#         if (count + b_size) <= len(df): \n",
    "#             batch_size = b_size\n",
    "#         else: \n",
    "#             batch_size = len(df) % b_size # incase when len(data) is not a multiple of batch_size \n",
    "\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "#             # getting names of all image frames in folder for i th video\n",
    "            \n",
    "#             ith_val = df.iloc[count]\n",
    "            \n",
    "#             f_name = df.iloc[count]['file_path']\n",
    "                \n",
    "#             frames_1 = frames_from_video_file(f_name, 30)\n",
    "#             # stacking all frames in the batch after scaling each frame \n",
    "#             all_frames = np.vstack((all_frames,(np.array(frames_1)/255)[np.newaxis,...])) \n",
    "\n",
    "#             # extracting the labels\n",
    "#             l = np.array([ith_val['extraversion'],\n",
    "#                           ith_val['neuroticism'],\n",
    "#                           ith_val['agreeableness'],\n",
    "#                           ith_val['conscientiousness'],\n",
    "#                           ith_val['openness']\n",
    "#                          ])\n",
    "            \n",
    "#             labels = np.vstack((labels,l[np.newaxis,...]))\n",
    "\n",
    "#             count += 1\n",
    "\n",
    "#         # after every epoch the counter must start again \n",
    "#         if count >= len(df): \n",
    "#             count = 0\n",
    "#             random.shuffle(df)\n",
    "\n",
    "#         yield all_frames, labels  # should yield audio and visual input of the model in one list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e06b24b-b44f-4090-8a6b-6710c632d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making objects for the train and validation generator\n",
    "# train_batches = get_batches(train_df, b_size = 8, train=True)\n",
    "# val_batches = get_batches(test_df, b_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2339fa1-a974-4aa4-bd38-f5cfe39d5b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65ced234-0bb3-45f6-ae2f-de2667988d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 30\n",
    "# n_frames = 5\n",
    "batch_size = 8\n",
    "# AUTO = tf.data.AUTOTUNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f593e4dd-29d2-493c-b1fb-9a47f0c93057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32, name=\"image\"),\n",
    "                    tf.TensorSpec(shape = (5,), dtype = tf.float32, name=\"labels\"))\n",
    "\n",
    "# # train_ds\n",
    "# train_ds = tf.data.Dataset.from_generator(FrameGenerator(train_df, n_frames, training=True), (tf.float32, tf.float32))\n",
    "# train_ds = train_ds.batch(batch_size)\n",
    "# # train_ds = train_ds\n",
    "\n",
    "# # val_ds\n",
    "# val_ds = tf.data.Dataset.from_generator(FrameGenerator(test_df, n_frames, training=False), (tf.float32, tf.float32))\n",
    "# val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "# # test data\n",
    "# test_ds = tf.data.Dataset.from_generator(FrameGenerator(val_annotation, n_frames, training=False), (tf.float32, tf.float32))\n",
    "# test_ds = test_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(MetaGenerator(train_df, n_frames, training=True), (tf.string, tf.float32))\n",
    "train_ds = train_ds.map(lambda x,y: frames_from_video_file_map(x, y, 30), tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(batch_size).repeat()\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(MetaGenerator(test_df, n_frames, training=False), (tf.string, tf.float32))\n",
    "val_ds = val_ds.map(lambda x,y: frames_from_video_file_map(x, y, 30), tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "# test data\n",
    "test_ds = tf.data.Dataset.from_generator(MetaGenerator(val_annotation, n_frames, training=False), (tf.string, tf.float32))\n",
    "test_ds = test_ds.map(lambda x,y: frames_from_video_file_map(x, y, 30),tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44924bd1-6875-4c5e-8861-816b06664bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b802d8ee-cc3f-4fb7-8502-bf8695c704af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for ele in train_ds.take(1):\n",
    "#     print(ele[1].numpy().shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b337ab64-9409-4f23-ae9e-bc163e88b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# reduce lr on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_acc', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='min', min_lr=0.0000000001)\n",
    "\n",
    "\n",
    "checkpoint = MyModelCheckpoint(os.path.join(run_dir, 'face-personality-model-{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "        monitor='val_loss',verbose=1, save_best_only=True, mode='auto')\n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2adaa-7999-4903-870b-5a5f8baf6ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c3fe4a8f-db41-4786-afa1-ce75ce89442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_env.predict(val_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c294dc-c77b-4b74-9d46-9cfeaa747800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:01:10.096708: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-01-31 15:01:10.097279: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2200185000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 15:01:18.854768: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-31 15:01:24.405744: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8500\n",
      "2023-01-31 15:01:26.995179: W tensorflow/stream_executor/gpu/asm_compiler.cc:191] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.0\n",
      "2023-01-31 15:01:26.995208: W tensorflow/stream_executor/gpu/asm_compiler.cc:194] Used ptxas at ptxas\n",
      "2023-01-31 15:01:26.995290: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Unimplemented: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-01-31 15:01:27.133505: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-31 15:01:27.134021: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-01-31 15:01:30.069152: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 249s 378ms/step - loss: 0.1216 - mean_acc: 0.8784 - val_loss: 0.1516 - val_mean_acc: 0.8484\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15164, saving model to ./checkpoint/face_personality/00008-face_personality/face-personality-model-01-0.15.h5\n",
      "\n",
      "Epoch 00001: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-01-0.15.pkl\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1191 - mean_acc: 0.8809 - val_loss: 0.1693 - val_mean_acc: 0.8307\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.15164\n",
      "\n",
      "Epoch 00002: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-02-0.17.pkl\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1192 - mean_acc: 0.8808 - val_loss: 0.1237 - val_mean_acc: 0.8763\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.15164 to 0.12373, saving model to ./checkpoint/face_personality/00008-face_personality/face-personality-model-03-0.12.h5\n",
      "\n",
      "Epoch 00003: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-03-0.12.pkl\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1191 - mean_acc: 0.8809 - val_loss: 0.1394 - val_mean_acc: 0.8606\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12373\n",
      "\n",
      "Epoch 00004: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-04-0.14.pkl\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 225s 376ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1237 - val_mean_acc: 0.8763\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12373 to 0.12369, saving model to ./checkpoint/face_personality/00008-face_personality/face-personality-model-05-0.12.h5\n",
      "\n",
      "Epoch 00005: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-05-0.12.pkl\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1191 - mean_acc: 0.8809 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00006: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-06-0.12.pkl\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00007: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-07-0.12.pkl\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1394 - val_mean_acc: 0.8606\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00008: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-08-0.14.pkl\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00009: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-09-0.12.pkl\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00010: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-10-0.12.pkl\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00011: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-11-0.12.pkl\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00012: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-12-0.12.pkl\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00013: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-13-0.12.pkl\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00014: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-14-0.12.pkl\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00015: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-15-0.12.pkl\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00016: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-16-0.12.pkl\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1675 - val_mean_acc: 0.8325\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00017: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-17-0.17.pkl\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00018: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-18-0.12.pkl\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00019: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-19-0.12.pkl\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00020: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-20-0.12.pkl\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 225s 376ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00021: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-21-0.12.pkl\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00022: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-22-0.12.pkl\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 225s 376ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00023: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-23-0.12.pkl\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00024: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-24-0.12.pkl\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00025: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-25-0.12.pkl\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00026: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-26-0.12.pkl\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00027: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-27-0.12.pkl\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 225s 375ms/step - loss: 0.1190 - mean_acc: 0.8810 - val_loss: 0.1238 - val_mean_acc: 0.8762\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12369\n",
      "\n",
      "Epoch 00028: saving optimizaer to ./checkpoint/face_personality/00008-face_personality/face-personality-model-28-0.12.pkl\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n",
      "Epoch 29/50\n",
      "498/600 [=======================>......] - ETA: 35s - loss: 0.1196 - mean_acc: 0.8804"
     ]
    }
   ],
   "source": [
    "model.fit(x= train_ds, steps_per_epoch=train_steps, \n",
    "               validation_data=val_ds,validation_steps=val_steps,\n",
    "               epochs=50, callbacks=callbacks_list)\n",
    "\n",
    "# model.fit(x= train_batches, steps_per_epoch=train_steps, \n",
    "#                validation_data=val_batches,validation_steps=val_steps,\n",
    "#                epochs=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71d911-e65c-4560-b36c-b05230b380e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu2.5",
   "language": "python",
   "name": "tfgpu2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
