{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "171f09d2-b2fe-488e-bac8-2bf29b107c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76dbf71b-75bb-424a-abb8-29225bf2ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6e07fd1d-2a7e-4933-a590-276b7f93b8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd009ac0-15b1-41f6-91b9-390dbe6c57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b65f2dae-9993-4d2b-9d81-a747ea431171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yamnet model\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)\n",
    "\n",
    "# vvgish model\n",
    "vggish_model_handle = \"https://tfhub.dev/google/vggish/1\"\n",
    "vggish_model = hub.load(vggish_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad64181-dd4d-4b98-9a26-1c7747597967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "54387bd0-98a2-4bd4-8e9e-958b76b5c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628af6e-2542-4fe0-9afe-1318ca6900b3",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97b5dd56-2779-49e1-96d6-d073c05e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and val data\n",
    "\n",
    "train_df = pd.read_csv(\"first_impression_audio_train.csv\")\n",
    "test_df = pd.read_csv(\"first_impression_audio_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61bcc232-298c-4495-8672-370691980a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9d110a3e-e4f8-426b-b246-901a172f755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)\n",
    "val_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0a60e7e-8bfe-45fd-b136-9e9c4b5515cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate x and ys i.e. data and latrain_test_split_name = train_df.file_path.values\n",
    "file_name = train_df.file_path\n",
    "targets = train_df[['openness',  'conscientiousness', 'extraversion', 'agreeableness','neuroticism']]\n",
    "\n",
    "# openness = train_df['openness']\n",
    "# cons = train_df['conscientiousness']\n",
    "# extr = train_df['extraversion']\n",
    "# agree = train_df['agreeableness']\n",
    "# neur = train_df['neuroticism']\n",
    "\n",
    "\n",
    "# val data\n",
    "val_file_name = val_df.file_path\n",
    "val_targets = val_df[['openness',  'conscientiousness', 'extraversion', 'agreeableness','neuroticism']]\n",
    "\n",
    "# val_openness = val_df['openness']\n",
    "# val_cons = val_df['conscientiousness']\n",
    "# val_extr = val_df['extraversion']\n",
    "# val_agree = val_df['agreeableness']\n",
    "# val_neur = val_df['neuroticism']\n",
    "\n",
    "\n",
    "# test data\n",
    "test_file_name = test_df.file_path\n",
    "test_targets = test_df[['openness',  'conscientiousness', 'extraversion', 'agreeableness','neuroticism']]\n",
    "\n",
    "# test_openness = test_df['openness']\n",
    "# test_cons = test_df['conscientiousness']\n",
    "# test_extr = test_df['extraversion']\n",
    "# test_agree = test_df['agreeableness']\n",
    "# test_neur = test_df['neuroticism']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cdf51d-8fce-4ee5-ad14-33d1722b3c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "376aaec7-b2e5-4630-914d-3d4a39293fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.reshape(test_targets[0], (-1, 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6da3caf2-8bfe-4a50-82ad-542f0ae68131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eea458ee-70ce-4f10-afe1-87486f281360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_wav_for_map(filename, label):\n",
    "#     return load_wav_16k_mono(filename), label\n",
    "#     # return load_wav_16k_mono(filename), tf.reshape(label, (-1, 5))\n",
    "\n",
    "    \n",
    "def load_wav_for_map(filename, label1, label2, label3, label4, label5):\n",
    "    return load_wav_16k_mono(filename), label1, label2, label3, label4, label5\n",
    "    # return load_wav_16k_mono(filename), tf.reshape(label, (-1, 5))\n",
    "\n",
    "\n",
    "def load_wav_for_map1(filename, label):\n",
    "    return load_wav_16k_mono(filename), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c00ce61c-de01-4d06-8979-34530b4d9993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tf dataset\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((file_name, targets))\n",
    "train_ds = train_ds.map(load_wav_for_map1, AUTO)\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((file_name, openness, cons, extr, agree, neur))\n",
    "# train_ds = train_ds.map(load_wav_for_map, AUTO)\n",
    "\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "47aa34be-6614-4501-830e-05234217aa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices((val_file_name, val_targets))\n",
    "val_ds = val_ds.map(load_wav_for_map1, AUTO)\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((val_file_name, val_openness, val_cons, val_extr, val_agree, val_neur))\n",
    "# val_ds = val_ds.map(load_wav_for_map, AUTO)\n",
    "\n",
    "\n",
    "val_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4d18c00-1217-4ca4-be71-d756c75e2345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((test_file_name, test_targets))\n",
    "test_ds = test_ds.map(load_wav_for_map1, AUTO)\n",
    "\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_file_name, test_openness, test_cons, test_extr, test_agree, test_neur))\n",
    "# test_ds = test_ds.map(load_wav_for_map, AUTO)\n",
    "\n",
    "\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "72cd2a16-adc7-40b5-a5fd-edbb42af0a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-2.1225111e-08  6.2852862e-08 -2.7310557e-08 ...  3.0617358e-03\n",
      "  5.4587917e-03  9.6694697e-03], shape=(244831,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for ele in train_ds:\n",
    "    # print(extract_embedding(ele[0].numpy(), 1))\n",
    "    print(ele[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "01fa9f13-f829-4eb7-b8dd-baf2df107234",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = val_batch_size = 8\n",
    "\n",
    "\n",
    "train_steps = len(train_df)//train_batch_size\n",
    "val_steps = len(val_df)//val_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec261844-d836-455c-a9df-d2bbe1225da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a5c626ad-5af5-4be0-9ff6-e9fcd75c4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label):\n",
    "    ''' run YAMNet to extract embedding from the wav data '''\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    print(embeddings)\n",
    "    \n",
    "    # embeddings = tf.math.reduce_mean(embeddings, axis=0)\n",
    "    \n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    # print(tf.shape(embeddings)[0])\n",
    "    \n",
    "    # use reduce mean to calculate mean of array along axis 1 (mean of column value)\n",
    "    return tf.math.reduce_mean(embeddings, axis=0), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aaefedb1-df7d-4d76-ac79-cbc47e83e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # applies the embedding extraction model to a wav data\n",
    "# def extract_embedding1(wav_data, label1, label2, label3, label4, label5):\n",
    "#     ''' run YAMNet to extract embedding from the wav data '''\n",
    "#     scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "#     print(embeddings)\n",
    "    \n",
    "#     embeddings = tf.math.reduce_mean(embeddings, axis=0)\n",
    "#     num_embeddings = tf.shape(embeddings)[0]\n",
    "#     print(tf.shape(embeddings)[0])\n",
    "#     return (\n",
    "#         embeddings,\n",
    "#         # spectrogram,\n",
    "#         # label\n",
    "#         tf.repeat(label1, num_embeddings),\n",
    "#         tf.repeat(label2, num_embeddings),\n",
    "#         tf.repeat(label3, num_embeddings),\n",
    "#         tf.repeat(label4, num_embeddings),\n",
    "#         tf.repeat(label5, num_embeddings)\n",
    "#            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c5aadd56-da86-40c5-8149-f0b3719219ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StatefulPartitionedCall:1\", shape=(None, 1024), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.map(extract_embedding)\n",
    "# train_ds = train_ds.map(extract_embedding1).unbatch()\n",
    "\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ae546769-f5fd-4063-b711-a0f4e0b80a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StatefulPartitionedCall:1\", shape=(None, 1024), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = val_ds.map(extract_embedding)\n",
    "\n",
    "# val_ds = val_ds.map(extract_embedding1).unbatch()\n",
    "val_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c449178b-0593-42ad-8a11-d46eb83be331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"StatefulPartitionedCall:1\", shape=(None, 1024), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = test_ds.map(extract_embedding)\n",
    "\n",
    "# test_ds = test_ds.map(extract_embedding1).unbatch()\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "447ae737-dd00-450f-b12c-7115121a212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # stack labels again\n",
    "# def concat_label(x, lb1, lb2, lb3, lb4, lb5):\n",
    "#     return x, tf.stack([lb1, lb2, lb3, lb4, lb5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8612241c-5c88-45ca-9a8a-7d7da2cb8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = train_ds.map(concat_label)\n",
    "# val_ds = val_ds.map(concat_label)\n",
    "# test_ds = test_ds.map(concat_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8187879e-f430-46f8-a03e-6ca2f316d4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.42222222 0.66990291 0.43925234 0.67032967 0.46875   ], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for ele in train_ds:\n",
    "    print(ele[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d9d3f823-1da9-4c5a-b89e-1e08a415a9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for ind,i in enumerate(train_ds):\n",
    "    print(type(i[1]))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa81f2-c892-42fb-bc11-87ff5a6de65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e1d98d80-04be-4991-845b-33668dad83cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # applies the embedding extraction model to a wav data\n",
    "# def extract_embedding1(wav_data):\n",
    "#     ''' run YAMNet to extract embedding from the wav data '''\n",
    "#     scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "#     print(embeddings)\n",
    "#     num_embeddings = tf.shape(embeddings)[0]\n",
    "#     # print(tf.shape(embeddings)[0])\n",
    "#     return (\n",
    "#         embeddings,\n",
    "#             # tf.repeat(label, num_embeddings)\n",
    "#            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c6d05-712d-4a13-9c18-8bc58487f471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bdd849f0-747c-43c7-8e4d-a1d6815bd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split x and y and extract audio feactures\n",
    "# train_ds1 = train_ds.map(lambda x,y: x)\n",
    "# target_dataset = train_ds.map(lambda x,y: y)\n",
    "# train_ds1 = train_ds1.map(extract_embedding1).unbatch()\n",
    "# train_ds1 = tf.data.Dataset.zip((train_ds1, target_dataset))\n",
    "\n",
    "\n",
    "# # val\n",
    "# val_ds1 = val_ds.map(lambda x,y: x)\n",
    "# val_target_dataset = val_ds.map(lambda x,y: y)\n",
    "\n",
    "# # test\n",
    "# test_ds1 = test_ds.map(lambda x,y: x)\n",
    "# test_target_dataset = test_ds.map(lambda x,y: y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd05baf-a27f-4c46-a23b-36b9a81860a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "89f02b9d-0595-4f46-a903-fa784ac33247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind, (x,y) in enumerate(train_ds1):\n",
    "#     print(x[0])\n",
    "#     print(f\"y = {y[0]}\")\n",
    "#     if ind == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "402d1db9-ad79-4f47-99cd-601510e403e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_ds = train_ds.map(extract_embedding)\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "14323fcd-a0cd-47cc-bfd5-eb13d8252dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = test_ds.map(extract_embedding)\n",
    "# val_ds = val_ds.map(extract_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecd9556a-8d37-4af3-b79e-bb277817c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.filter(lambda x,y :x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d2d9a30d-31bc-4889-a9e5-b72959a9da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a8e5fb3c-23e8-4bf3-9da1-feeb2dc33459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds1 = train_ds.unbatch().batch(train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4aa8d5b2-01bf-484e-ba6d-448aaceb7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j,i in enumerate(train_ds1):\n",
    "#     print(i[0].shape)\n",
    "#     print(i[0].numpy().shape)\n",
    "#     # print(i[1].shape)\n",
    "#     if j ==10:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "56a0db9b-757c-4f43-a9b7-e763a6d41172",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(32).repeat().batch(train_batch_size).prefetch(AUTO)\n",
    "val_ds = val_ds.batch(val_batch_size).prefetch(AUTO)\n",
    "test_ds = test_ds.batch(val_batch_size).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ae2cfd10-7efe-4407-9405-d6e586b39d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in train_ds.take(1):\n",
    "#     print(i[0].shape)\n",
    "#     # tf.keras.layers.GlobalAveragePooling2D(i[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8c7e2706-6fdd-4129-ba35-5d1099061269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    # tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='sigmoid')\n",
    "], name='audio_personality')\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24deea5b-4bda-4e52-a4a3-9e6636139d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "23319279-613e-4149-a14f-8a6f5b49f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b52f8aa7-4224-4382-be7d-abf8b39e03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, axis=0, **kwargs):\n",
    "#         super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "#         self.axis = axis\n",
    "        \n",
    "#     def call(self, input):\n",
    "#         return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ccf41cde-5c6d-4350-ab30-e762b2e092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "# embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "#                                             trainable=False, name='yamnet')\n",
    "# _, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "# serving_outputs = base_model(embeddings_output)\n",
    "# serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "# model = tf.keras.Model(input_segment, serving_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e59ac1e5-eb00-42a6-9909-56b9fb98c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in train_ds.take(1):\n",
    "#     _, emb, _ = embedding_extraction_layer(i[0])\n",
    "#     op=base_model(emb)\n",
    "#     print(ReduceMeanLayer(axis=0, name='classifier')(op))\n",
    "#     print(j[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ab215-f2a5-4ce7-838a-2dfffb23a776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13be1142-de96-4bd9-b061-2f97245d5961",
   "metadata": {},
   "source": [
    "# Custom callback to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7f743087-7643-4ac3-8326-fa409d0028e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_dir(outdir, run_desc):\n",
    "    prev_run_dirs = []\n",
    "    if os.path.isdir(outdir):\n",
    "        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(\\\n",
    "            os.path.join(outdir, x))]\n",
    "    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n",
    "    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n",
    "    cur_run_id = max(prev_run_ids, default=-1) + 1\n",
    "    run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{run_desc}')\n",
    "    assert not os.path.exists(run_dir)\n",
    "    os.makedirs(run_dir)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fedb0713-d790-4007-a7b2-efd782fe00aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ./checkpoint/audio_personality/00008-audio_personality\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "outdir = \"./checkpoint/audio_personality/\"\n",
    "if not os.path.isdir(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "run_desc = \"audio_personality\"\n",
    "\n",
    "run_dir = generate_output_dir(outdir, run_desc)\n",
    "print(f\"Results saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698141e-12d6-421e-a91e-c6d8e068b2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ca1a5c27-1579-4547-8342-c75d8365b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import pickle\n",
    "\n",
    "\n",
    "class MyModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch,logs)\n",
    "\n",
    "        # Also save the optimizer state\n",
    "        filepath = self._get_file_path(epoch=epoch, \n",
    "                                       logs=logs\n",
    "                                       # ,batch=None\n",
    "                                      )\n",
    "\n",
    "        filepath = filepath.rsplit( \".\", 1 )[ 0 ] \n",
    "        filepath += \".pkl\"\n",
    "\n",
    "        with open(filepath, 'wb') as fp:\n",
    "            pickle.dump(\n",
    "            {\n",
    "                'opt': model.optimizer.get_config(),\n",
    "                'epoch': epoch+1,\n",
    "                'lr': model.optimizer.learning_rate\n",
    "                \n",
    "             # Add additional keys if you need to store more values\n",
    "            }, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('\\nEpoch %05d: saving optimizaer to %s' % (epoch + 1, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bde1b286-0fe0-450f-b948-7a469d9e3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# reduce lr on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_acc', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='min', min_lr=0.0000000001)\n",
    "\n",
    "\n",
    "checkpoint = MyModelCheckpoint(os.path.join(run_dir, 'audio-personality-model-{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "        monitor='val_loss',verbose=1, save_best_only=True, mode='auto')\n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5942bcb-1480-46b5-8242-d8dc7e36eb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eb86f3ea-8dd0-4b10-a0a7-0c655d8b9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "def mean_acc(y_true, y_pred):\n",
    "    diff = k.abs(y_true - y_pred)\n",
    "    return k.mean(1-diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f6a8b039-06df-4bb0-a141-c8e515f2190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = ['mae'],\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "    metrics = [mean_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fe28e245-642a-4c13-9ed4-2c0c37283366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"audio_personality\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 527,365\n",
      "Trainable params: 527,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612951a-299c-4674-9f48-1a76e1c49951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:02:19.032132: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/675 [..............................] - ETA: 3:39:00 - loss: 0.1340 - mean_acc: 0.8660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:02:30.725627: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-02-02 13:02:30.725742: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1050s 2s/step - loss: 0.1121 - mean_acc: 0.8879 - val_loss: 0.1123 - val_mean_acc: 0.8877\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11233, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-01-0.11.h5\n",
      "\n",
      "Epoch 00001: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-01-0.11.pkl\n",
      "Epoch 2/50\n",
      "675/675 [==============================] - 1032s 2s/step - loss: 0.1075 - mean_acc: 0.8925 - val_loss: 0.1092 - val_mean_acc: 0.8908\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11233 to 0.10918, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-02-0.11.h5\n",
      "\n",
      "Epoch 00002: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-02-0.11.pkl\n",
      "Epoch 3/50\n",
      "675/675 [==============================] - 1028s 2s/step - loss: 0.1057 - mean_acc: 0.8943 - val_loss: 0.1087 - val_mean_acc: 0.8913\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10918 to 0.10866, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-03-0.11.h5\n",
      "\n",
      "Epoch 00003: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-03-0.11.pkl\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 4/50\n",
      "675/675 [==============================] - 1022s 2s/step - loss: 0.1036 - mean_acc: 0.8964 - val_loss: 0.1075 - val_mean_acc: 0.8925\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10866 to 0.10749, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-04-0.11.h5\n",
      "\n",
      "Epoch 00004: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-04-0.11.pkl\n",
      "Epoch 5/50\n",
      "675/675 [==============================] - 1016s 2s/step - loss: 0.1028 - mean_acc: 0.8972 - val_loss: 0.1069 - val_mean_acc: 0.8931\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10749 to 0.10689, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-05-0.11.h5\n",
      "\n",
      "Epoch 00005: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-05-0.11.pkl\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 6/50\n",
      "675/675 [==============================] - 994s 1s/step - loss: 0.1017 - mean_acc: 0.8983 - val_loss: 0.1064 - val_mean_acc: 0.8936\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10689 to 0.10639, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-06-0.11.h5\n",
      "\n",
      "Epoch 00006: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-06-0.11.pkl\n",
      "Epoch 7/50\n",
      "675/675 [==============================] - 953s 1s/step - loss: 0.1014 - mean_acc: 0.8986 - val_loss: 0.1064 - val_mean_acc: 0.8936\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10639\n",
      "\n",
      "Epoch 00007: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-07-0.11.pkl\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 8/50\n",
      "675/675 [==============================] - 936s 1s/step - loss: 0.1007 - mean_acc: 0.8993 - val_loss: 0.1061 - val_mean_acc: 0.8939\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10639 to 0.10610, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-08-0.11.h5\n",
      "\n",
      "Epoch 00008: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-08-0.11.pkl\n",
      "Epoch 9/50\n",
      "675/675 [==============================] - 935s 1s/step - loss: 0.1005 - mean_acc: 0.8995 - val_loss: 0.1061 - val_mean_acc: 0.8939\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10610 to 0.10609, saving model to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-09-0.11.h5\n",
      "\n",
      "Epoch 00009: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-09-0.11.pkl\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "Epoch 10/50\n",
      "675/675 [==============================] - 937s 1s/step - loss: 0.1002 - mean_acc: 0.8998 - val_loss: 0.1061 - val_mean_acc: 0.8939\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10609\n",
      "\n",
      "Epoch 00010: saving optimizaer to ./checkpoint/audio_personality/00008-audio_personality/audio-personality-model-10-0.11.pkl\n",
      "Epoch 11/50\n",
      "480/675 [====================>.........] - ETA: 3:46 - loss: 0.1001 - mean_acc: 0.8999"
     ]
    }
   ],
   "source": [
    "model.fit(x= train_ds, steps_per_epoch=train_steps, \n",
    "               validation_data=val_ds,validation_steps=val_steps,\n",
    "               epochs=50, callbacks=callbacks_list)\n",
    "\n",
    "# model.fit(x= train_batches, steps_per_epoch=train_steps, \n",
    "#                validation_data=val_batches,validation_steps=val_steps,\n",
    "#                epochs=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd1e88-2f0e-4937-a2af-f2700628865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling into single model\n",
    "\n",
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, axis=0, **kwargs):\n",
    "        super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        \n",
    "    def call(self, input):\n",
    "        return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c17a22-2aac-439a-9a63-c10d29e75ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100081f-9502-4e58-a12d-367717aed6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f9065-45e2-4eba-97e1-852f50c6ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_model_path = './audio_personality_yamnet'\n",
    "\n",
    "# input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "# embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "#                                             trainable=True, name='yamnet')\n",
    "# _, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "# serving_outputs = model(embeddings_output)\n",
    "# serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "# serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
    "# # serving_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67874e1b-537e-4b45-acc0-5ecac0875646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serving_model(load_wav_16k_mono(test_df.file_path[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496da0c-0a5a-4afb-bdbe-3845993efbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.neuroticism[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f51e3b-5e45-47d4-892b-0c17794c70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serving_model.compile(\n",
    "#     loss = ['mae'],\n",
    "#     # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "#     metrics = [mean_acc]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3524d8-b5f4-4faf-8f26-d245d0687a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_ds = tf.data.Dataset.from_tensor_slices(train_df.file_path.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f09679-39d6-49a6-8a92-280b016a3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_ds = tr_ds.map(load_wav_16k_mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b3652-9a59-4b0a-b8e1-8852f649b3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971767e-ff54-4d91-8a0f-c4aa68484286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serving_model.fit(x=tr_ds, steps_per_epoch=train_steps, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983595e-962c-450d-83e6-a37474f0bd24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu2.5",
   "language": "python",
   "name": "tfgpu2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
