{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f62ab89-88f5-48df-877d-a1db1ed39293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 10:32:17.085495: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ab9285f-9524-42ee-9555-bb53b346dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 10:32:19.065105: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-02-02 10:32:19.199432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2023-02-02 10:32:19.199511: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-02-02 10:32:19.204811: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-02-02 10:32:19.204890: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-02-02 10:32:19.206066: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-02 10:32:19.206418: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-02 10:32:19.207129: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-02-02 10:32:19.208539: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-02-02 10:32:19.208724: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-02-02 10:32:19.380706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785821ae-aa4a-4420-b9ad-a6d2f0a800c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install fvcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6c6fd-d197-40d2-bbf5-86f9052a12f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adffd3c-736e-41f9-87ad-f057dbdf9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7554257f-e3e5-44e8-a878-8cec7ddfdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Choose the `slow_r50` model \n",
    "\n",
    "# model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd08d657-0f67-4a42-8421-982153be5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import urllib\n",
    "# from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "# from torchvision.transforms import Compose, Lambda\n",
    "# from torchvision.transforms._transforms_video import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9c9da7-8f50-4151-b8ef-7bc26a19572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set to GPU or CPU\n",
    "# device = \"cpu\"\n",
    "# model = model.eval()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1541e1-6386-44e9-9f16-63d60317eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "# json_filename = \"kinetics_classnames.json\"\n",
    "# try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "# except: urllib.request.urlretrieve(json_url, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68b0911-3c11-4667-9df0-8c58307b162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(json_filename, \"r\") as f:\n",
    "#     kinetics_classnames = json.load(f)\n",
    "\n",
    "# # Create an id to label name mapping\n",
    "# kinetics_id_to_classname = {}\n",
    "# for k, v in kinetics_classnames.items():\n",
    "#     kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeac0ef0-7d76-496c-9f5d-2e029eabe214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side_size = 256\n",
    "# mean = [0.45, 0.45, 0.45]\n",
    "# std = [0.225, 0.225, 0.225]\n",
    "# crop_size = 256\n",
    "# num_frames = 8\n",
    "# sampling_rate = 8\n",
    "# frames_per_second = 30\n",
    "\n",
    "# # Note that this transform is specific to the slow_R50 model.\n",
    "# transform =  ApplyTransformToKey(\n",
    "#     key=\"video\",\n",
    "#     transform=Compose(\n",
    "#         [\n",
    "#             UniformTemporalSubsample(num_frames),\n",
    "#             Lambda(lambda x: x/255.0),\n",
    "#             NormalizeVideo(mean, std),\n",
    "#             ShortSideScale(\n",
    "#                 size=side_size\n",
    "#             ),\n",
    "#             CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "#         ]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # The duration of the input clip is also specific to the model.\n",
    "# clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f735d634-e170-402a-8f91-3900a6571596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
    "# video_path = 'archery.mp4'\n",
    "# try: urllib.URLopener().retrieve(url_link, video_path)\n",
    "# except: urllib.request.urlretrieve(url_link, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07672184-d9d3-4d93-8eb4-604c1ad6fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = \"/mount1/mount1/First-Impression/Apparent_traits/DeepImpression/deep_impression/_fJcW5234bY.001.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d8890c-3727-417a-b125-709748110065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the duration of the clip to load by specifying the start and end duration\n",
    "# # The start_sec should correspond to where the action occurs in the video\n",
    "# start_sec = 0\n",
    "# end_sec = start_sec + clip_duration\n",
    "\n",
    "# # Initialize an EncodedVideo helper class and load the video\n",
    "# video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# # Load the desired clip\n",
    "# video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e20a17c-828e-409b-91e6-81e297b7d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0b334a-cee1-4ca4-99a6-78842cbbe31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Apply a transform to normalize the video input\n",
    "# video_data = transform(video_data)\n",
    "\n",
    "# # Move the inputs to the desired device\n",
    "# inputs = video_data[\"video\"]\n",
    "# inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c147d433-4ed2-4e0a-8c09-8062d296f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pass the input clip through the model\n",
    "# preds = model(inputs[None, ...])\n",
    "\n",
    "# # Get the predicted classes\n",
    "# post_act = torch.nn.Softmax(dim=1)\n",
    "# preds = post_act(preds)\n",
    "# pred_classes = preds.topk(k=5).indices[0]\n",
    "\n",
    "# # Map the predicted classes to the label names\n",
    "# pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
    "# print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db03bf12-9765-4923-9b58-9e00fa5728df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# cnt  =0\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "# while True:\n",
    "#     ret, fr = cap.read()\n",
    "#     if ret:\n",
    "#         cnt += 1\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d4ad8d0-a83b-4c0d-83a7-fb5fc6e4fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e45bad6-b6d4-4fb7-a0c7-bb1f37d5b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be6d46-dc44-426d-a509-8d8d88165345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6000c50-a5e0-4fe0-8bd2-4d46f22b3714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 10:32:19.592761: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 10:32:19.759975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2023-02-02 10:32:19.764827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-02-02 10:32:19.764943: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-02-02 10:32:25.914858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-02 10:32:25.914894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-02-02 10:32:25.914912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-02-02 10:32:26.086868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 38317 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0)\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet101V2(include_top=False, weights=\"imagenet\",input_shape=(224,224, 3), pooling='max')\n",
    "# base_model.trainble = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "382f8d53-498d-4c8d-95e3-4a87b8a3edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04c64bc6-1f2b-48a9-b143-bf3f2ea1aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "emb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77c85ffa-361e-4077-96b1-46f1a0515a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(video_pth, seq_len=30, target_resolution=(224, 224)):\n",
    "    \"\"\"\n",
    "    extract frames and audio from the video,\n",
    "    store the cropped frames and audio file in the output folders\n",
    "    seq_len: how many frames will be extracted from the video.\n",
    "              Considering all videos from this dataset have similar duration\n",
    "              video_duration = seq_len / fps\n",
    "    target_resolution: (desired_height, desired_width) of the facial frame extracted\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    video = VideoFileClip(video_pth, target_resolution=target_resolution)\n",
    "    \n",
    "    times = list(np.arange(0, video.duration, video.duration/seq_len))\n",
    "    if len(times) < seq_len:\n",
    "        times.append(video.duration)\n",
    "    times = times[:seq_len]\n",
    "\n",
    "    for i, t in enumerate(times):\n",
    "        img = cv2.cvtColor(video.get_frame(t), cv2.COLOR_BGR2RGB)\n",
    "        frames.append(img)\n",
    "        img1 = np.expand_dims(img, 0)\n",
    "        # print(img1.shape)\n",
    "        em = base_model.predict(img1)\n",
    "        emb.append(em)\n",
    "    print(\"Video duration {} seconds. Extracted {} frames\".format(video.duration, len(times)))\n",
    "    \n",
    "# preprocess(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b61fef3d-06fe-4afa-a69f-39e8116b62b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43ba64b4-5af6-4b01-9623-50cfd5075d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 30\n",
    "h = w =224\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55da9c-b54d-4db5-a70b-7f6fbc741013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "affc6e41-2aaf-46cc-8c18-bd2929e668ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeDistributed Layer is used to run resnet model first on all the frames and then run LSTM on top of it\n",
    "input_layer_env = Input(shape=(time_step, h, w, c))\n",
    "\n",
    "tm_layer_env = TimeDistributed(base_model)(input_layer_env)\n",
    "\n",
    "lstm_env = LSTM(256)(tm_layer_env)\n",
    "\n",
    "op_env =  Dense(5, activation='sigmoid')(lstm_env)\n",
    "\n",
    "model_env = Model(input_layer_env, op_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6556c480-42d2-4e65-aaba-ac2dd2ffd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as k\n",
    "def mean_acc(y_true, y_pred):\n",
    "    diff = k.abs(y_true - y_pred)\n",
    "    return k.mean(1-diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76012503-8c34-40a5-8db5-2d295c30a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env.compile(\n",
    "    loss = ['mae'],\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "    metrics = [mean_acc]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "807373bd-24fb-474e-930f-8644d2753acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 30, 224, 224, 3)] 0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 30, 2048)          42626560  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               2360320   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 44,988,165\n",
      "Trainable params: 44,890,501\n",
      "Non-trainable params: 97,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_env.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0075b-7e10-4bb4-9ed1-d18b11dace30",
   "metadata": {},
   "source": [
    "# Dataset preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa9cf18-e215-4cee-9c02-8099a519b31c",
   "metadata": {},
   "source": [
    "## Training data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9838a47-d78f-45a6-bf1b-3fed60f0a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_file_path(fl):\n",
    "    train_fol = \"/mnt/FirstImpression/unzippedData/**/*.mp4\"\n",
    "    train_fol = glob(train_fol)\n",
    "    train_fol = [i for i in train_fol if \"training\" in Path(i).parts[-2]]\n",
    "    fil = list(filter(lambda x: fl in x, train_fol))\n",
    "    return fil[0]\n",
    "#     for i in train_fol:\n",
    "#         if fl in i:\n",
    "#             fil = i\n",
    "            \n",
    "#     return fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58bbe46f-58f8-4cc3-a342-be4f5616e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_annot = pd.read_pickle(\"/mnt/FirstImpression/annotation_training.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d55fb6d8-c96c-40e9-8dc0-c95453a66bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation = pd.DataFrame().from_dict(train_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1132d89-68e9-4484-9fa9-b80eb0efb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation['file_name'] = train_annotation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93cc8c92-3888-4303-9acf-f41f43be609c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>J4GQm9j0JZ0.003.mp4</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>J4GQm9j0JZ0.003.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zEyRyTnIw5I.005.mp4</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>zEyRyTnIw5I.005.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nskJh7v6v1U.004.mp4</th>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>nskJh7v6v1U.004.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6wHQsN5g2RM.000.mp4</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>6wHQsN5g2RM.000.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dQOeQYWIgm8.000.mp4</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>dQOeQYWIgm8.000.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eh7WRYXVh9M.000.mp4</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>Eh7WRYXVh9M.000.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2q8orkMs2Jg.003.mp4</th>\n",
       "      <td>0.728972</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>2q8orkMs2Jg.003.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1lAPYh4t3U.000.mp4</th>\n",
       "      <td>0.700935</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>F1lAPYh4t3U.000.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cxJ0u6r0-pU.001.mp4</th>\n",
       "      <td>0.317757</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>cxJ0u6r0-pU.001.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hfUH9Am-Izs.000.mp4</th>\n",
       "      <td>0.401869</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>hfUH9Am-Izs.000.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     extraversion  neuroticism  agreeableness  \\\n",
       "J4GQm9j0JZ0.003.mp4      0.523364     0.552083       0.626374   \n",
       "zEyRyTnIw5I.005.mp4      0.345794     0.375000       0.472527   \n",
       "nskJh7v6v1U.004.mp4      0.252336     0.291667       0.406593   \n",
       "6wHQsN5g2RM.000.mp4      0.457944     0.489583       0.505495   \n",
       "dQOeQYWIgm8.000.mp4      0.607477     0.489583       0.406593   \n",
       "...                           ...          ...            ...   \n",
       "Eh7WRYXVh9M.000.mp4      0.523364     0.479167       0.626374   \n",
       "2q8orkMs2Jg.003.mp4      0.728972     0.760417       0.582418   \n",
       "F1lAPYh4t3U.000.mp4      0.700935     0.770833       0.747253   \n",
       "cxJ0u6r0-pU.001.mp4      0.317757     0.531250       0.582418   \n",
       "hfUH9Am-Izs.000.mp4      0.401869     0.500000       0.461538   \n",
       "\n",
       "                     conscientiousness  interview  openness  \\\n",
       "J4GQm9j0JZ0.003.mp4           0.601942   0.504673  0.488889   \n",
       "zEyRyTnIw5I.005.mp4           0.582524   0.457944  0.366667   \n",
       "nskJh7v6v1U.004.mp4           0.485437   0.373832  0.511111   \n",
       "6wHQsN5g2RM.000.mp4           0.398058   0.457944  0.377778   \n",
       "dQOeQYWIgm8.000.mp4           0.621359   0.570093  0.622222   \n",
       "...                                ...        ...       ...   \n",
       "Eh7WRYXVh9M.000.mp4           0.621359   0.588785  0.544444   \n",
       "2q8orkMs2Jg.003.mp4           0.524272   0.616822  0.822222   \n",
       "F1lAPYh4t3U.000.mp4           0.699029   0.691589  0.788889   \n",
       "cxJ0u6r0-pU.001.mp4           0.679612   0.616822  0.588889   \n",
       "hfUH9Am-Izs.000.mp4           0.543689   0.429907  0.588889   \n",
       "\n",
       "                               file_name  \n",
       "J4GQm9j0JZ0.003.mp4  J4GQm9j0JZ0.003.mp4  \n",
       "zEyRyTnIw5I.005.mp4  zEyRyTnIw5I.005.mp4  \n",
       "nskJh7v6v1U.004.mp4  nskJh7v6v1U.004.mp4  \n",
       "6wHQsN5g2RM.000.mp4  6wHQsN5g2RM.000.mp4  \n",
       "dQOeQYWIgm8.000.mp4  dQOeQYWIgm8.000.mp4  \n",
       "...                                  ...  \n",
       "Eh7WRYXVh9M.000.mp4  Eh7WRYXVh9M.000.mp4  \n",
       "2q8orkMs2Jg.003.mp4  2q8orkMs2Jg.003.mp4  \n",
       "F1lAPYh4t3U.000.mp4  F1lAPYh4t3U.000.mp4  \n",
       "cxJ0u6r0-pU.001.mp4  cxJ0u6r0-pU.001.mp4  \n",
       "hfUH9Am-Izs.000.mp4  hfUH9Am-Izs.000.mp4  \n",
       "\n",
       "[6000 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84a230fc-26ea-43c0-b385-228f7168c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_annotation['file_path'] = train_annotation['file_name'].progress_apply(lambda x: get_train_file_path(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "336f6965-96b6-4ab0-b35b-e91678fb43ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_annotation.to_csv(\"first_impression_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a69c467a-d2aa-4537-b288-5986a8d8fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation = pd.read_csv(\"first_impression_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef428d1a-0912-484f-825b-13686ffc2a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>J4GQm9j0JZ0.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.472527</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>zEyRyTnIw5I.005.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.485437</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>nskJh7v6v1U.004.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>6wHQsN5g2RM.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>dQOeQYWIgm8.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>Eh7WRYXVh9M.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>0.728972</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>2q8orkMs2Jg.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>0.700935</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>F1lAPYh4t3U.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>0.317757</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.679612</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>cxJ0u6r0-pU.001.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>0.401869</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>hfUH9Am-Izs.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/training80_4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      extraversion  neuroticism  agreeableness  conscientiousness  interview  \\\n",
       "0         0.523364     0.552083       0.626374           0.601942   0.504673   \n",
       "1         0.345794     0.375000       0.472527           0.582524   0.457944   \n",
       "2         0.252336     0.291667       0.406593           0.485437   0.373832   \n",
       "3         0.457944     0.489583       0.505495           0.398058   0.457944   \n",
       "4         0.607477     0.489583       0.406593           0.621359   0.570093   \n",
       "...            ...          ...            ...                ...        ...   \n",
       "5995      0.523364     0.479167       0.626374           0.621359   0.588785   \n",
       "5996      0.728972     0.760417       0.582418           0.524272   0.616822   \n",
       "5997      0.700935     0.770833       0.747253           0.699029   0.691589   \n",
       "5998      0.317757     0.531250       0.582418           0.679612   0.616822   \n",
       "5999      0.401869     0.500000       0.461538           0.543689   0.429907   \n",
       "\n",
       "      openness            file_name  \\\n",
       "0     0.488889  J4GQm9j0JZ0.003.mp4   \n",
       "1     0.366667  zEyRyTnIw5I.005.mp4   \n",
       "2     0.511111  nskJh7v6v1U.004.mp4   \n",
       "3     0.377778  6wHQsN5g2RM.000.mp4   \n",
       "4     0.622222  dQOeQYWIgm8.000.mp4   \n",
       "...        ...                  ...   \n",
       "5995  0.544444  Eh7WRYXVh9M.000.mp4   \n",
       "5996  0.822222  2q8orkMs2Jg.003.mp4   \n",
       "5997  0.788889  F1lAPYh4t3U.000.mp4   \n",
       "5998  0.588889  cxJ0u6r0-pU.001.mp4   \n",
       "5999  0.588889  hfUH9Am-Izs.000.mp4   \n",
       "\n",
       "                                              file_path  \n",
       "0     /mnt/FirstImpression/unzippedData/training80_6...  \n",
       "1     /mnt/FirstImpression/unzippedData/training80_7...  \n",
       "2     /mnt/FirstImpression/unzippedData/training80_5...  \n",
       "3     /mnt/FirstImpression/unzippedData/training80_3...  \n",
       "4     /mnt/FirstImpression/unzippedData/training80_7...  \n",
       "...                                                 ...  \n",
       "5995  /mnt/FirstImpression/unzippedData/training80_5...  \n",
       "5996  /mnt/FirstImpression/unzippedData/training80_5...  \n",
       "5997  /mnt/FirstImpression/unzippedData/training80_6...  \n",
       "5998  /mnt/FirstImpression/unzippedData/training80_1...  \n",
       "5999  /mnt/FirstImpression/unzippedData/training80_4...  \n",
       "\n",
       "[6000 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fb9daec-5938-4cfa-b53a-af28d8c09c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "\n",
    "# with open(\"./first_impression_train.pkl\", 'wb') as f:\n",
    "#     pkl.dump(train_annot, f)\n",
    "\n",
    "# # pd.to_pickle(\"./first_impression_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddfa3383-cf9a-4f8f-a137-012a505c3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_pth, output_pth, seq_len=30, target_resolution=(224, 224)):\n",
    "    \"\"\"\n",
    "    extract frames and audio from the video,\n",
    "    store the cropped frames and audio file in the output folders\n",
    "    seq_len: how many frames will be extracted from the video.\n",
    "              Considering all videos from this dataset have similar duration\n",
    "              video_duration = seq_len / fps\n",
    "    target_resolution: (desired_height, desired_width) of the facial frame extracted\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    video = VideoFileClip(video_pth, target_resolution=target_resolution)\n",
    "    \n",
    "    times = list(np.arange(0, video.duration, video.duration/seq_len))\n",
    "    if len(times) < seq_len:\n",
    "        times.append(video.duration)\n",
    "    times = times[:seq_len]\n",
    "    \n",
    "    for i, t in enumerate(times):\n",
    "        img = cv2.cvtColor(video.get_frame(t), cv2.COLOR_BGR2RGB)\n",
    "        # frames.append(img)\n",
    "        # img1 = np.expand_dims(img, 0)\n",
    "        # print(img1.shape)\n",
    "        # em = base_model.predict(img1)\n",
    "        # emb.append(em)\n",
    "        cv2.imwrite(f\"{output_pth}/frame_{i}.jpg\", img)\n",
    "    # print(\"Video duration {} seconds. Extracted {} frames\".format(video.duration, len(times)))\n",
    "\n",
    "# fet_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d45200a-4e65-4c7f-9b85-92f1863198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# from tqdm import tqdm\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "\n",
    "# # make training folder\n",
    "# train_folder = \"training_frame_folder\"\n",
    "\n",
    "# if not os.path.isdir(train_folder):\n",
    "#     os.mkdir(train_folder)\n",
    "\n",
    "# else:\n",
    "#     train_fol = \"/mount1/mount1/First-Impression/unzippedData/**/*.mp4\"\n",
    "#     train_fol = glob(train_fol)\n",
    "#     train_fol = [i for i in train_fol if \"training\" in Path(i).parts[-2]]\n",
    "    \n",
    "#     for fl in tqdm(train_fol):\n",
    "#         img_folder = train_folder+\"/\" + Path(fl).parts[-1].split(\".\")[0]\n",
    "        \n",
    "#         # print(img_folder)\n",
    "        \n",
    "#         if not os.path.isdir(img_folder):\n",
    "#             os.mkdir(img_folder)\n",
    "#         get_frames(video_pth=fl, output_pth=img_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd95e5d-5aba-4d2e-acd6-402fefc60ab5",
   "metadata": {},
   "source": [
    "## val data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d892c6-b355-4cc8-8bb7-974d801f5238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1caf951-95d0-410e-9be0-ba6036402221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_file_path(fl):\n",
    "    train_fol = \"/mnt/FirstImpression/unzippedData/**/*.mp4\"\n",
    "    train_fol = glob(train_fol)\n",
    "    train_fol = [i for i in train_fol if \"validation\" in Path(i).parts[-2]]\n",
    "    fil = list(filter(lambda x: fl in x, train_fol))\n",
    "    return fil[0]\n",
    "#     for i in train_fol:\n",
    "#         if fl in i:\n",
    "#             fil = i\n",
    "            \n",
    "#     return fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6205c95a-7f35-4a1b-a2ad-f2fd64b33b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_annot = pd.read_pickle(\"/mnt/FirstImpression/annotation_validation.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49752d07-5c06-4286-8c7d-b436730dbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotation = pd.DataFrame().from_dict(val_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ab5cca8-98a3-4de1-a8cd-4f9979ac471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotation['file_name'] = val_annotation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23f2be18-f466-4fe6-b90e-22f9262b0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_annotation['file_path'] = val_annotation['file_name'].progress_apply(lambda x: get_val_file_path(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4fdfc06-2906-4b40-ad59-51b18882e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_annotation.to_csv(\"first_impression_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed40af2f-86c0-406c-8384-a3bc63e40f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_annotation = pd.read_csv(\"first_impression_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cebf0fd-6eef-4160-bda4-85af49f21ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>interview</th>\n",
       "      <th>openness</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>modNfUPt3F4.002.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>h6LOjpCRXtY.005.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>WER4ww680QQ.004.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.364486</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.553398</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>c4XnKouozXU.002.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345794</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.516484</td>\n",
       "      <td>0.417476</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>OEKg-Tvwcbk.002.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.614583</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.689320</td>\n",
       "      <td>0.626168</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>3LAaFUSGvsU.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>n2BuwHbdilY.000.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.551402</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.504673</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>GcuoyJPO-KU.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.551402</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>uf_sIIw4zxY.004.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.560748</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.635514</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>jd9_8OPxM3A.003.mp4</td>\n",
       "      <td>/mnt/FirstImpression/unzippedData/validation80...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      extraversion  neuroticism  agreeableness  conscientiousness  interview  \\\n",
       "0         0.644860     0.593750       0.615385           0.640777   0.616822   \n",
       "1         0.439252     0.520833       0.417582           0.572816   0.439252   \n",
       "2         0.457944     0.312500       0.428571           0.398058   0.373832   \n",
       "3         0.364486     0.572917       0.527473           0.553398   0.523364   \n",
       "4         0.345794     0.468750       0.516484           0.417476   0.383178   \n",
       "...            ...          ...            ...                ...        ...   \n",
       "1995      0.570093     0.614583       0.494505           0.689320   0.626168   \n",
       "1996      0.542056     0.541667       0.549451           0.669903   0.579439   \n",
       "1997      0.551402     0.593750       0.560440           0.572816   0.504673   \n",
       "1998      0.514019     0.552083       0.461538           0.572816   0.551402   \n",
       "1999      0.560748     0.635417       0.725275           0.621359   0.635514   \n",
       "\n",
       "      openness            file_name  \\\n",
       "0     0.555556  modNfUPt3F4.002.mp4   \n",
       "1     0.411111  h6LOjpCRXtY.005.mp4   \n",
       "2     0.555556  WER4ww680QQ.004.mp4   \n",
       "3     0.322222  c4XnKouozXU.002.mp4   \n",
       "4     0.477778  OEKg-Tvwcbk.002.mp4   \n",
       "...        ...                  ...   \n",
       "1995  0.577778  3LAaFUSGvsU.000.mp4   \n",
       "1996  0.666667  n2BuwHbdilY.000.mp4   \n",
       "1997  0.644444  GcuoyJPO-KU.003.mp4   \n",
       "1998  0.733333  uf_sIIw4zxY.004.mp4   \n",
       "1999  0.666667  jd9_8OPxM3A.003.mp4   \n",
       "\n",
       "                                              file_path  \n",
       "0     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "2     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "3     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "4     /mnt/FirstImpression/unzippedData/validation80...  \n",
       "...                                                 ...  \n",
       "1995  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1996  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1997  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1998  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "1999  /mnt/FirstImpression/unzippedData/validation80...  \n",
       "\n",
       "[2000 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91469b2c-b855-49ce-96c2-214433f7c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "\n",
    "# with open(\"./first_impression_val.pkl\", 'wb') as f:\n",
    "#     pkl.dump(val_annot, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ddccf-2499-4cb8-8dff-66dc00748638",
   "metadata": {},
   "source": [
    "# Custom callback to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98d2eac6-5a9b-425b-88eb-2e60f1a7fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_dir(outdir, run_desc):\n",
    "    prev_run_dirs = []\n",
    "    if os.path.isdir(outdir):\n",
    "        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(\\\n",
    "            os.path.join(outdir, x))]\n",
    "    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n",
    "    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n",
    "    cur_run_id = max(prev_run_ids, default=-1) + 1\n",
    "    run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{run_desc}')\n",
    "    assert not os.path.exists(run_dir)\n",
    "    os.makedirs(run_dir)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14886ae0-3164-4839-a20c-8afafc96d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ./checkpoint/00005-env_personality\n"
     ]
    }
   ],
   "source": [
    "outdir = \"./checkpoint/\"\n",
    "run_desc = \"env_personality\"\n",
    "\n",
    "run_dir = generate_output_dir(outdir, run_desc)\n",
    "print(f\"Results saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7885df8a-530e-40f5-9331-e9ebd55c3523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "class MyModelCheckpoint(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch,logs)\n",
    "\n",
    "        # Also save the optimizer state\n",
    "        filepath = self._get_file_path(epoch=epoch, \n",
    "                                       logs=logs, \n",
    "                                       # batch=None\n",
    "                                      )\n",
    "\n",
    "        filepath = filepath.rsplit( \".\", 1 )[ 0 ] \n",
    "        filepath += \".pkl\"\n",
    "\n",
    "        with open(filepath, 'wb') as fp:\n",
    "            pickle.dump(\n",
    "            {\n",
    "                'opt': model_env.optimizer.get_config(),\n",
    "                'epoch': epoch+1,\n",
    "                'lr': model_env.optimizer.learning_rate\n",
    "                \n",
    "             # Add additional keys if you need to store more values\n",
    "            }, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print('\\nEpoch %05d: saving optimizaer to %s' % (epoch + 1, filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c776da79-f6f5-4c89-b718-4f3454836499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# reduce lr on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_acc', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='min', min_lr=0.0000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea19598-be73-4438-9b03-37e1de3783b1",
   "metadata": {},
   "source": [
    "# Tf dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4dbe20c-cc38-4866-8455-9d3d905ddca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "        Pad and resize an image from a video.\n",
    "\n",
    "        Args:\n",
    "          frame: Image that needs to resized and padded. \n",
    "          output_size: Pixel size of the output frame image.\n",
    "\n",
    "        Return:\n",
    "          Formatted frame with padding of specified output size.\n",
    "    \"\"\"\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return tf.keras.applications.resnet_v2.preprocess_input(frame)\n",
    "    # return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b26b406-00d8-4deb-8ad9-cc740ad55d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "    \"\"\"\n",
    "        Creates frames from each video file present for each category.\n",
    "\n",
    "        Args:\n",
    "          video_path: File path to the video.\n",
    "          n_frames: Number of frames to be created per video file.\n",
    "          output_size: Pixel size of the output frame image.\n",
    "\n",
    "        Return:\n",
    "          An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    result.append(format_frames(frame, output_size))\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = format_frames(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "    src.release()\n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529e377-6806-44f6-bb72-aa730ae1508a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da5101a6-6850-48ec-ba99-e4b5b5208116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_from_video_file(video_path, 30).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4047c5ed-558a-408e-a4fa-1fbb61feed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample df to test model working\n",
    "df = train_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "163d887c-0768-4fdd-8de5-bd70de6ab019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3669d710-872e-4899-a888-cf6ed55b9986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0471e465-068f-40f8-ac75-4e53e5fc4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dcce6e9d-5146-4f02-8a56-49964d887932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61e48bf6-4856-4003-bd5c-055d65729680",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d34c8a8-e2c2-4cc6-91b5-b37b93bfbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f7a7a36-e4ca-4e03-a580-9f964a56afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fr_and_label_from_video(video_path,label):\n",
    "#     \"\"\"\n",
    "#         Creates frames from each video file present for each category.\n",
    "\n",
    "#         Args:\n",
    "#           video_path: File path to the video.\n",
    "#           n_frames: Number of frames to be created per video file.\n",
    "#           output_size: Pixel size of the output frame image.\n",
    "\n",
    "#         Return:\n",
    "#           An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     n_frames=30\n",
    "#     output_size = (224,224)\n",
    "#     frame_step = 15\n",
    "    \n",
    "#     # print(tf.strings.as_string(video_path))\n",
    "#     print(video_path)\n",
    "#     print(label)\n",
    "    \n",
    "#     # Read each video frame by frame\n",
    "#     result = []\n",
    "#     src = cv2.VideoCapture(str(video_path))  \n",
    "#     print(src.isOpened())\n",
    "#     video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "#     need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "#     if need_length > video_length:\n",
    "#         start = 0\n",
    "#     else:\n",
    "#         max_start = video_length - need_length\n",
    "#         start = random.randint(0, max_start + 1)\n",
    "\n",
    "#     src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "#     # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "#     ret, frame = src.read()\n",
    "#     result.append(format_frames(frame, output_size))\n",
    "#     for _ in range(n_frames - 1):\n",
    "#         for _ in range(frame_step):\n",
    "#             ret, frame = src.read()\n",
    "#         if ret:\n",
    "#             frame = format_frames(frame, output_size)\n",
    "#             result.append(frame)\n",
    "#         else:\n",
    "#             result.append(np.zeros_like(result[0]))\n",
    "#     src.release()\n",
    "#     result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "#     return result, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c12fab-ca85-4aa2-9846-df8eb0dfcf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6173ed6-8d57-41c5-a3d7-89151c5c7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seperate x_train  and y_train\n",
    "\n",
    "# y_train = train_df[['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']].values\n",
    "# x_train = train_df.file_path.values\n",
    "\n",
    "# tr_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d313c7d-f9b1-4da9-8ce1-93016952fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = val_batch_size = 8\n",
    "\n",
    "\n",
    "train_steps = len(train_df)//train_batch_size\n",
    "val_steps = len(test_df)//val_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7ba4516-847f-48b1-8b89-21f62f9ce5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf dataset generator\n",
    "class FrameGenerator:\n",
    "    def __init__(self, df, n_frames, training = False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "            Args:\n",
    "                path: Video file paths.\n",
    "                n_frames: Number of frames. \n",
    "                training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "  \n",
    "\n",
    "    def __call__(self):\n",
    "        if self.training:\n",
    "            import sklearn\n",
    "            self.df = sklearn.utils.shuffle(self.df)\n",
    "            \n",
    "        for files in self.df.itertuples():\n",
    "            video_frames = frames_from_video_file(files.file_path, self.n_frames)\n",
    "            openness = files.openness\n",
    "            conscientiousness = files.conscientiousness\n",
    "            extraversion = files.extraversion\n",
    "            agreeableness = files.agreeableness\n",
    "            neuroticism = files.neuroticism\n",
    "            \n",
    "            label = np.array([files.openness, files.conscientiousness ,files.extraversion, files.agreeableness, files.neuroticism])\n",
    "            \n",
    "            yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71a0a3-09b7-4998-a53b-5c21e76a6125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "facc7d33-4a11-4f1a-9c4f-216d842ba73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_ds.take(10):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ad7ddcd-7b3d-4b4f-8650-e4906ab30ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_steps = 30\n",
    "\n",
    "\n",
    "# def get_batches(df, b_size = 16, train=False):\n",
    "#     count = 0 # counter variable \n",
    "    \n",
    "#     if train:\n",
    "#         import sklearn\n",
    "#         df = sklearn.utils.shuffle(df)\n",
    "            \n",
    "#     # looping over all videos\n",
    "#     while count < len(df): \n",
    "#         all_frames = np.empty((0,time_steps,h,w,c)) # numpy array of batch of videos \n",
    "#         labels = np.empty((0,5)) # numpy array to contain the 5 personality scores \n",
    "\n",
    "#         if (count + b_size) <= len(df): \n",
    "#             batch_size = b_size\n",
    "#         else: \n",
    "#             batch_size = len(df) % b_size # incase when len(data) is not a multiple of batch_size \n",
    "\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "#             # getting names of all image frames in folder for i th video\n",
    "            \n",
    "#             ith_val = df.iloc[count]\n",
    "            \n",
    "#             f_name = df.iloc[count]['file_path']\n",
    "                \n",
    "#             frames_1 = frames_from_video_file(f_name, 30)\n",
    "#             # stacking all frames in the batch after scaling each frame \n",
    "#             all_frames = np.vstack((all_frames,(np.array(frames_1)/255)[np.newaxis,...])) \n",
    "\n",
    "#             # extracting the labels\n",
    "#             l = np.array([ith_val['extraversion'],\n",
    "#                           ith_val['neuroticism'],\n",
    "#                           ith_val['agreeableness'],\n",
    "#                           ith_val['conscientiousness'],\n",
    "#                           ith_val['openness']\n",
    "#                          ])\n",
    "            \n",
    "#             labels = np.vstack((labels,l[np.newaxis,...]))\n",
    "\n",
    "#             count += 1\n",
    "\n",
    "#         # after every epoch the counter must start again \n",
    "#         if count >= len(df): \n",
    "#             count = 0\n",
    "#             random.shuffle(df)\n",
    "\n",
    "#         yield all_frames, labels  # should yield audio and visual input of the model in one list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0e06b24b-b44f-4090-8a6b-6710c632d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making objects for the train and validation generator\n",
    "# train_batches = get_batches(train_df, b_size = 8, train=True)\n",
    "# val_batches = get_batches(test_df, b_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4806f2ea-8190-4383-8c12-554e8e30fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.Dataset.from_generator(get_batches(train_df, b_size = 8, train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17b071f1-ff3f-4d09-a8ad-63f205b265b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39d05927-5a34-4b9f-92df-903b7c743abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e474ead2-ff69-42a6-bab6-6bd20b3a96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ccab7c07-25bf-46ab-8cff-75bfbbf1618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_env.fit(train_batches, validation_data=val_batches, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2339fa1-a974-4aa4-bd38-f5cfe39d5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf dataset generator\n",
    "class MetaGenerator:\n",
    "    def __init__(self, df, n_frames, training = False):\n",
    "        \"\"\" Returns a set of frames with their associated label. \n",
    "            Args:\n",
    "                path: Video file paths.\n",
    "                n_frames: Number of frames. \n",
    "                training: Boolean to determine if training dataset is being created.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "  \n",
    "\n",
    "    def __call__(self):\n",
    "        if self.training:\n",
    "            import sklearn\n",
    "            self.df = sklearn.utils.shuffle(self.df)\n",
    "            \n",
    "        for files in self.df.itertuples():\n",
    "            # video_frames = frames_from_video_file(files.file_path, self.n_frames)\n",
    "            video_frames = files.file_path\n",
    "            openness = files.openness\n",
    "            conscientiousness = files.conscientiousness\n",
    "            extraversion = files.extraversion\n",
    "            agreeableness = files.agreeableness\n",
    "            neuroticism = files.neuroticism\n",
    "            \n",
    "            label = np.array([files.openness, files.conscientiousness ,files.extraversion, files.agreeableness, files.neuroticism])\n",
    "            \n",
    "            yield video_frames, label\n",
    "            # return video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9efe5b96-c336-494d-bdc7-b3dc2b93a4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frames_from_video_file_map(video_path, label, n_frames, output_size = (224,224), frame_step = 15):\n",
    "#     \"\"\"\n",
    "#         Creates frames from each video file present for each category.\n",
    "\n",
    "#         Args:\n",
    "#           video_path: File path to the video.\n",
    "#           n_frames: Number of frames to be created per video file.\n",
    "#           output_size: Pixel size of the output frame image.\n",
    "\n",
    "#         Return:\n",
    "#           An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "#     \"\"\"\n",
    "#     # Read each video frame by frame\n",
    "#     result = []\n",
    "#     src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "#     video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "#     need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "#     if need_length > video_length:\n",
    "#         start = 0\n",
    "#     else:\n",
    "#         max_start = video_length - need_length\n",
    "#         start = random.randint(0, max_start + 1)\n",
    "\n",
    "#     src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "#     # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "#     ret, frame = src.read()\n",
    "#     result.append(format_frames(frame, output_size))\n",
    "#     for _ in range(n_frames - 1):\n",
    "#         for _ in range(frame_step):\n",
    "#             ret, frame = src.read()\n",
    "#         if ret:\n",
    "#             frame = format_frames(frame, output_size)\n",
    "#             result.append(frame)\n",
    "#         else:\n",
    "#             result.append(np.zeros_like(result[0]))\n",
    "#     src.release()\n",
    "#     result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "#     return result, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00bf7d2b-caef-4667-9c41-20cb1e051c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def frames_from_video_file_map(video_path, label, n_frames, output_size = (224,224), frame_step = 15):\n",
    "    \"\"\"\n",
    "        Creates frames from each video file present for each category.\n",
    "\n",
    "        Args:\n",
    "          video_path: File path to the video.\n",
    "          n_frames: Number of frames to be created per video file.\n",
    "          output_size: Pixel size of the output frame image.\n",
    "\n",
    "        Return:\n",
    "          An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "    \"\"\"\n",
    "    # Read each video frame by frame\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "    ret, frame = src.read()\n",
    "    if ret:\n",
    "        st = time()\n",
    "        \n",
    "        ft_time = time()\n",
    "        result.append(format_frames(frame, output_size))\n",
    "    else:\n",
    "        er_t = time()\n",
    "        result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "        # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "\n",
    "    s = time()\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            ft_time = time()\n",
    "            fr = format_frames(frame, output_size)\n",
    "            result.append(fr)\n",
    "\n",
    "        else:\n",
    "            er_t = time()\n",
    "            result.append(np.zeros_like(np.random.rand(224, 224, 3)))\n",
    "            # print(f\"Append random array of req shape:- {time() - er_t}\")\n",
    "                \n",
    "    # print(f\"Time for getting aligned face from n-1 frame:- {time() - s}\")\n",
    "    src.release()\n",
    "    if len(result) < n_frames:\n",
    "        print(len(result))\n",
    "    \n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "    \n",
    "    return result, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65ced234-0bb3-45f6-ae2f-de2667988d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 30\n",
    "batch_size = 8\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32, name=\"image\"),\n",
    "                    tf.TensorSpec(shape = (5,), dtype = tf.float32, name=\"labels\"))\n",
    "\n",
    "# # train_ds\n",
    "# train_ds = tf.data.Dataset.from_generator(FrameGenerator(train_df, n_frames, training=True), (tf.float32, tf.float32))\n",
    "# train_ds = train_ds.batch(batch_size)\n",
    "# # train_ds = train_ds\n",
    "\n",
    "# # val_ds\n",
    "# val_ds = tf.data.Dataset.from_generator(FrameGenerator(test_df, n_frames, training=False), (tf.float32, tf.float32))\n",
    "# val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "# # test data\n",
    "# test_ds = tf.data.Dataset.from_generator(FrameGenerator(val_annotation, n_frames, training=False), (tf.float32, tf.float32))\n",
    "# test_ds = test_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "# Used tf dataset from_generator to get metadata and map method to process video frames to decrease training time\n",
    "train_ds = tf.data.Dataset.from_generator(MetaGenerator(train_df, n_frames, training=True), (tf.string, tf.float32))\n",
    "train_ds = train_ds.map(lambda x,y: frames_from_video_file_map(x, y, 30), tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(batch_size).repeat()\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(MetaGenerator(test_df, n_frames, training=False), (tf.string, tf.float32))\n",
    "val_ds = val_ds.map(lambda x,y: frames_from_video_file_map(x, y, 30), tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "# test data\n",
    "test_ds = tf.data.Dataset.from_generator(MetaGenerator(val_annotation, n_frames, training=False), (tf.string, tf.float32))\n",
    "test_ds = test_ds.map(lambda x,y: frames_from_video_file_map(x, y, 30),tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b802d8ee-cc3f-4fb7-8502-bf8695c704af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ele in train_ds.take(1):\n",
    "#     print(ele[1].numpy().shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b337ab64-9409-4f23-ae9e-bc163e88b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = MyModelCheckpoint(os.path.join(run_dir, 'env-personality-model-{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "        monitor='val_loss',verbose=1, save_best_only=True, mode='auto')\n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3f2adaa-7999-4903-870b-5a5f8baf6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model state\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "def load_model_data(model_path, opt_path):\n",
    "    model = load_model(model_path)\n",
    "    with open(opt_path, 'rb') as fp:\n",
    "        d = pickle.load(fp)\n",
    "        epoch = d['epoch']\n",
    "        opt = d['opt']\n",
    "    return epoch, model, opt\n",
    "\n",
    "# epoch, model_env, opt = load_model_data('/mount1/harish/Personality_trait/checkpoint/00000-env_personality/env-personality-model-01-0.12.h5',\n",
    "#                                         '/mount1/harish/Personality_trait/checkpoint/00000-env_personality/env-personality-model-01-0.12.pkl')\n",
    "\n",
    "# model_env.compile(\n",
    "#     loss = ['mae'],\n",
    "#     # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "#     metrics = [mean_acc]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c56e981b-b8a4-45e8-898d-111b18c22f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env.fit(x= train_ds, steps_per_epoch=train_steps, \n",
    "              validation_data=val_ds, validation_steps=val_steps,\n",
    "              epochs=20, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c3fe4a8f-db41-4786-afa1-ce75ce89442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_env.predict(val_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92c294dc-c77b-4b74-9d46-9cfeaa747800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial code to train model\n",
    "\n",
    "# model_env.fit(x= train_ds, steps_per_epoch=train_steps, validation_data=val_ds, \n",
    "#               validation_steps=val_steps,epochs=50, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7b71d911-e65c-4560-b36c-b05230b380e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = load_model(\n",
    "#     \"env_personality.h5\",\n",
    "#     custom_objects={\"mean_acc\": mean_acc}\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4fe60fbe-4394-43f5-8d2e-4b1f68b1dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "73d06059-932b-4dcf-b567-11fbbe0264e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29343c5d-5da4-49f7-9aac-79d5c7916579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in test_ds:\n",
    "#     print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b46b35-5c49-412a-a8a4-8b9dec44c931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu2.5",
   "language": "python",
   "name": "tfgpu2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
